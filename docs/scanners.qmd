---
title: Scanners
---

## Overview

Scanners are the main unit of processing in Inspect Scout and can target a wide variety of content types. In this article we'll cover the basic scanning concepts, and then drill into creating scanners that target various types (`Transcript`, `ChatMessage`, or `Event`) as well as creating custom loaders which enable scanning of lists of events or messages.

This article goes in depth on custom scanner development. If you are looking for a straightforward high-level way to create an LLM-based scanner see the [LLM Scanner](llm_scanner.qmd) documentation.

Notet that you can also use scanners directly as Inspect scorers (see [Scanners as Scorers](#scanners-as-scorers) for details).

## Scanner Basics

A `Scanner` is a function that takes a `ScannerInput` (typically a `Transcript`, but possibly an `Event`, `ChatMessage`, or list of events or messages) and returns a `Result`.

The result includes a `value` which can be of any type—this might be `True` to indicate that something was found but might equally be a number to indicate a count. More elaborate scanner values (`dict` or `list`) are also possible.

Here is a simple scanner that uses a model to look for agent "confusion"—whether or not it finds confusion, it still returns the model completion as an `explanation`:

``` python
@scanner(messages="all")
def confusion() -> Scanner[Transcript]:
    
    async def scan(transcript: Transcript) -> Result:

        # call model
        output = await get_model().generate(
            "Here is a transcript of an LLM agent " +
            "solving a puzzle:\n\n" +
            "===================================" +
            await messages_as_str(transcript) +
            "===================================\n\n" +
            "In the transcript above do you see the " +
            "agent becoming confused? Respond " +
            "beginning with 'Yes' or 'No', followed " +
            "by an explanation."
        )

        # extract the first word
        match = re.match(r"^\w+", output.completion.strip())

        # return result
        if match:
            answer = match.group(0)
            return Result(
                value=answer.lower() == "yes",
                answer=answer,
                explanation=output.completion,
            )
        else:
            return Result(value=False, explanation=output.completion)

    return scan
```

This scanner illustrates some of the lower-level mechanics of building custom scanners. You can also use the higher level `llm_scanner()` to implement this in far fewer lines of code:

``` python
from inspect_scout import Transcript, llm_scanner, scanner

@scanner(messages="all")
def confusion() -> Scanner[Transcript]:
    return llm_scanner(
        question="In the transcript above do you see " +
            "the agent becoming confused?"
        answer="boolean"
    )
```

### Input Types

`Transcript` is the most common `ScannerInput` however several other types are possible:

-   `Event` — Single event from the transcript (e.g. `ModelEvent`, `ToolEvent`, etc.).

-   `ChatMessage` — Single chat message from the transcript message history.

-   `list[Event]` or `list[ChatMessage]` — Arbitrary sets of events or messages extracted from the `Transcript` (see [Loaders](#loaders) below for details).

See the sections on [Transcripts](#transcripts), [Events](#events), [Messages](#messages), and [Loaders](#loaders) below for additional details on handling various input types.

### Input Filtering

One important principle of the Inspect Scout transcript pipeline is that only the precise data to be scanned should be read, and nothing more. This can dramatically improve performance as messages and events that won't be seen by scanners are never deserialized. Scanner input filters are specified as arguments to the `@scanner` decorator (you may have noticed the `messages="all"` attached to the scanner decorator in the example above).

For example, here we are looking for instances of assistants swearing---for this task we only need to look at assistant messages so we specify `messages=["assistant"]`

``` python
@scanner(messages=["assistant"])
def assistant_swearing() -> Scanner[Transcript]:

    async def scan(transcript: Transcript) -> Result:
        swear_words = [
            word 
            for m in transcript.messages 
            for word in extract_swear_words(m.text)
        ]
        return Result(
            value=len(swear_words),
            explanation=",".join(swear_words)
        )

    return scan
```

With this filter, only assistant messages (and no events at all) will be loaded from transcripts during scanning.

Note that by default, no filters are active, so if you don't specify values for `messages` and/or `events` your scanner will not be called!

## Transcripts {#transcripts}

Transcripts are the most common input to scanners. If you are reading from Inspect eval logs, each log will have `samples * epochs` transcripts. If you are reading from another source, each agent trace will yield a single `Transcript`.

### Transcript Fields

{{< include _transcript_fields.md >}}

### Content Filtering

Note that the `messages` and `events` fields will not be populated unless you specify a `messages` or `events` filter on your scanner. For example, this scanner will see all messages and events:

``` python
@scanner(messages="all", events="all")
def my_scanner() -> Scanner[Transcript]: ...
```

This scanner will see only model and tool events:

``` python
@scanner(events=["model", "tool"])
def my_scanner() -> Scanner[Transcript]: ...
```

This scanner will see only assistant messages:

``` python
@scanner(messages=["assistant"])
def my_scanner() -> Scanner[Transcript]: ...
```

### Presenting Messages

When processing transcripts, you will often want to present an entire message history to model for analysis. Above, we used the `messages_as_str()` function to do this:

``` python
# call model
result = await get_model().generate(
    "Here is a transcript of an LLM agent " +
    "solving a puzzle:\n\n" +
    "===================================" +
    await messages_as_str(transcript) +
    "===================================\n\n" +
    "In the transcript above do you see the agent " +
    "becoming confused? Respond beginning with 'Yes' " +
    "or 'No', followed by an explanation."
)
```

The `messages_as_str()` function takes a `Transcript | list[ChatMessage]` and will by default remove system messages from the message list. See `MessagesPreprocessor` for other available options.

## Multiple Results {#multiple-results}

Scanners can return multiple results as a list. For example:

``` python
return [
    Result(label="deception", value=True, explanation="..."),
    Result(label="misconfiguration", value=True, explanation="...")
]
```

This is useful when a scanner is capable of making several types of observation. In this case it's also important to indicate the origin of the result (i.e. which class of observation is is), which you can do using the `label` field (note that `label` can repeat multiple times in a set, so e.g. you could have multiple results with `label="deception"`).

When a list is returned, each individual result will yield its own row in the [results data frame](results.qmd#data-frames).

When validating scanners that return lists of results, you can use [result set validation](validation.qmd#result-set-validation) to specify expected values for each label independently.

## Event Scanners {#events}

To write a scanner that targets events, write a function that takes the event type(s) you want to process. For example, this scanner will see only model events:

``` python
@scanner
def my_scanner() -> Scanner[ModelEvent]:
    def scan(event: ModelEvent) -> Result: 
        ...

    return scan
```

Note that the `events="model"` filter was not required since we had already declared our scanner to take only model events. If we wanted to take both model and tool events we'd do this:

``` python
@scanner
def my_scanner() -> Scanner[ModelEvent | ToolEvent]:
    def scan(event: ModelEvent | ToolEvent) -> Result: 
        ...

    return scan
```

## Message Scanners {#messages}

To write a scanner that targets messages, write a function that takes the message type(s) you want to process. For example, this scanner will only see tool messages:

``` python
@scanner
def my_scanner() -> Scanner[ChatMessageTool]:
    def scan(message: ChatMessageTool) -> Result: 
        ...

    return scan
```

This scanner will see only user and assistant messages:

``` python
@scanner
def my_scanner() -> Scanner[ChatMessageUser | ChatMessageAssistant]:
    def scan(message: ChatMessageUser | ChatMessageAssistant) -> Result: 
        ...

    return scan
```

## Scanner Metrics

{{< include _scanner_metrics.md >}}


### Result Sets

If your scanner yields [multiple results](#multiple-results) you can still use it as a scorer, but you will want to provide a dictionary of metrics corresponding to the labels used by your results. For example, if you have a scanner that can yield results with `label="deception"` or `label="misconfiguration"`, you might declare your metrics like this:

``` python
@scanner(messages="all", metrics=[{ "deception": [mean(), stderr()], "misconfiguration": [mean(), stderr()] }])
def my_scanner() -> Scanner[Transcript]: ...
```

Or you can use a glob (\*) to use the same metrics for all labels:

``` python
@scanner(messages="all", metrics=[{ "*": [mean(), stderr()] }])
def my_scanner() -> Scanner[Transcript]: ...
```

You should also be sure to return a result for each supported label (so that metrics can be computed correctly on each row).

## Packaging {#packaging}

A convenient way to distribute scanners is to include them in a Python package. This makes it very easy for others to use your scanner and ensure they have all of the required dependencies.

Scanners in packages can be *registered* such that users can easily refer to them by name from the CLI. For example, if your package is named `myscanners` and your scanner is named `reward_hacking` you could do a scan with:

```bash
scout scan myscanners/reward_hacking
```

### Example

Here's an example that walks through all of the requirements for registering scanners in packages. Let's say your package is named `myscanners` and has a task named `reward_hacking` in the `scanners.py` file:

```         
myscanners/       
  myscanners/
    scanners.py
    _registry.py
  pyproject.toml
```

The `_registry.py` file serves as a place to import things that you want registered with Inspect. For example:

``` {.python filename="_registry.py"}
from .scanners import reward_hacking
```

You can then register `reward_hacking` (and anything else imported into `_registry.py`) as a [setuptools entry point](https://setuptools.pypa.io/en/latest/userguide/entry_point.html). This will ensure that inspect can resolve references to your package from the CLI. Here is how this looks in `pyproject.toml`:

::: {.panel-tabset group="entry-points"}
## Setuptools

``` toml
[project.entry-points.inspect_ai]
myscanners = "myscanners._registry"
```

## uv

``` toml
[project.entry-points.inspect_ai]
myscanners = "myscanners._registry"
```

## Poetry

``` toml
[tool.poetry.plugins.inspect_ai]
myscanners = "myscanners._registry"
```
:::

Now, anyone that has installed your package can run use your scanner as follows:

``` bash
scout scan myscanners/reward_hacking
```


## Scanning Toolkit {#scanning-toolkit}

The scanning toolkit provides composable functions for building custom scanning pipelines—the same building blocks that [`llm_scanner()`](llm_scanner.qmd) uses internally. When `llm_scanner` doesn't fit your needs (custom prompting strategy, multi-pass scanning, non-standard result aggregation), drop down to these APIs.

### Incremental Example

Here's how the toolkit functions compose, starting simple and adding capabilities:

**Step 1: Basic scanning.** Use `message_numbering()` to render messages with `[M1]`, `[M2]`... prefixes, then `generate_answer()` to call the LLM and parse the response:

``` python
from inspect_scout import (
    Transcript, Result, message_numbering, generate_answer, scanner,
)

@scanner(messages="all")
def my_scanner():
    async def scan(transcript: Transcript) -> Result:
        messages_as_str, extract_references = message_numbering()
        text = await messages_as_str(transcript.messages)
        prompt = f"Analyze this transcript:\n\n{text}\n\nDid the agent succeed?"
        return await generate_answer(
            prompt, "boolean", extract_references=extract_references,
        )
    return scan
```

**Step 2: Context-aware segmentation.** Wrap with `transcript_messages()` to handle transcripts that exceed the model's context window:

``` python
from inspect_ai.model import get_model
from inspect_scout import (
    Transcript, Result, message_numbering, generate_answer,
    transcript_messages, scanner,
)

@scanner(messages="all")
def my_scanner():
    async def scan(transcript: Transcript) -> Result:
        messages_as_str, extract_references = message_numbering()
        model = get_model()
        results = []
        async for segment in transcript_messages(
            transcript, messages_as_str=messages_as_str, model=model,
        ):
            prompt = f"Analyze this transcript:\n\n{segment.text}\n\nDid the agent succeed?"
            result = await generate_answer(
                prompt, "boolean", extract_references=extract_references,
            )
            results.append(result)
        return results[-1] if results else Result(value=False)
    return scan
```

**Step 3: Parallel scanning.** Replace the manual loop with `scan_segments()` for concurrent generation:

``` python
from inspect_ai.model import get_model
from inspect_scout import (
    Transcript, Result, message_numbering, generate_answer,
    transcript_messages, scan_segments, ResultReducer, scanner,
)

@scanner(messages="all")
def my_scanner():
    async def scan(transcript: Transcript) -> Result:
        messages_as_str, extract_references = message_numbering()
        model = get_model()

        async def scan_one(segment):
            prompt = f"Analyze this transcript:\n\n{segment.text}\n\nDid the agent succeed?"
            return await generate_answer(
                prompt, "boolean", extract_references=extract_references,
            )

        results = await scan_segments(
            transcript_messages(transcript, messages_as_str=messages_as_str, model=model),
            scan_one,
        )
        return await ResultReducer.any(results) if len(results) > 1 else results[0]
    return scan
```

### Message Numbering

`message_numbering()` creates a linked pair of functions with shared state:

``` python
from inspect_scout import message_numbering, MessagesPreprocessor

messages_as_str, extract_references = message_numbering(
    preprocessor=MessagesPreprocessor(exclude_system=True)  # optional
)
```

The returned `messages_as_str(messages)` renders a message list as a numbered string with `[M1]`, `[M2]`... prefixes. The counter auto-increments across calls—if the first call renders `[M1]`–`[M5]`, the next starts at `[M6]`. The returned `extract_references(text)` resolves `[M14]`-style citations from any prior `messages_as_str` call back to their original message identifiers.

``` python
# First call: [M1]...[M5]
text1 = await messages_as_str(first_messages)

# Second call: [M6]...[M10]  (continuous numbering)
text2 = await messages_as_str(second_messages)

# Resolves citations from either call
refs = extract_references("The issue appears in [M3] and [M8]")
```

### Message Extraction

`transcript_messages()` is the high-level entry point for extracting scannable message segments from a transcript. It automatically selects the best strategy based on what data is available:

- **Timelines present** → walks the span tree via `timeline_messages()`
- **Events present** → segments with compaction handling
- **Messages only** → context window segmentation

``` python
from inspect_ai.model import get_model
from inspect_scout import transcript_messages, message_numbering

messages_as_str, extract_references = message_numbering()
model = get_model()

async for segment in transcript_messages(
    transcript,
    messages_as_str=messages_as_str,
    model=model,
    context_window=128_000,  # override detected window
    compaction="all",        # "all" or "last"
    depth=2,                 # limit timeline tree depth
):
    # segment.messages — list[ChatMessage]
    # segment.text — rendered string with [M1]... prefixes
    # segment.segment — segment index (0-based)
    ...
```

For lower-level control:

| Function | Description |
|----------|-------------|
| `segment_messages(source, ...)` | Segments a message list, event list, or span into context-window-sized chunks. |
| `span_messages(source, *, compaction)` | Extracts raw messages from a `TimelineSpan` or event list, handling compaction merging. |

: {tbl-colwidths=\[35,65\]}

### Answer Generation

`generate_answer()` is a one-step function that constructs the LLM call, handles structured vs. text generation, retries on refusals, and parses the response into a `Result`:

``` python
from inspect_scout import generate_answer

result = await generate_answer(
    prompt="Analyze this transcript:\n\n{text}\n\nDid the agent succeed?",
    answer="boolean",
    extract_references=extract_references,  # from message_numbering()
)
# result.value: bool
# result.answer: str ("yes" or "no")
# result.explanation: str
# result.references: list[Reference]
```

For structured answers, `generate_answer` automatically uses tool-based generation. Set `parse=False` to get the raw `ModelOutput` instead of a parsed `Result`.

For custom generation pipelines, use `parse_answer()` to handle only the parsing step:

``` python
from inspect_scout import parse_answer

# After your own model call...
result = parse_answer(output, answer="boolean", extract_references=extract_references)
```

The `answer` parameter accepts any `AnswerSpec`:

| Type | Description |
|------|-------------|
| `"boolean"` | Yes/no answer → `bool` value |
| `"numeric"` | Numeric answer → `float` value |
| `"string"` | Free-text answer → `str` value |
| `list[str]` | Single-label classification |
| `AnswerMultiLabel` | Multi-label classification |
| `AnswerStructured` | Structured JSON via Pydantic model |

: {tbl-colwidths=\[25,75\]}

### Scanning Timelines

A timeline is a tree of **spans** representing the structure of an agent's execution. Each span corresponds to an agent, tool, or scorer invocation and contains its own events (model calls, tool calls). Timelines are auto-detected from transcript events: agent hierarchy, conversation threads, branches, and utility agents.

`timeline_messages()` walks the span tree and yields `TimelineMessages` segments (which extend `MessagesSegment` with an additional `.span` field):

``` python
from inspect_ai.model import get_model
from inspect_scout import timeline_messages, message_numbering

messages_as_str, extract_references = message_numbering()
model = get_model()

async for tm in timeline_messages(
    transcript.timelines[0],
    messages_as_str=messages_as_str,
    model=model,
):
    # tm.span — the TimelineSpan being scanned
    # tm.text — rendered messages for this span
    # tm.segment — segment index within this span
    print(f"Scanning span: {tm.span.name}")
```

The `include` parameter controls which spans are visited:

| Value | Behavior |
|-------|----------|
| `None` (default) | Non-utility spans that contain direct `ModelEvent`s |
| `str` | Spans whose name matches (case-insensitive) |
| `callable` | Custom predicate: `(TimelineSpan) -> bool` |

: {tbl-colwidths=\[25,75\]}

Here is an example of a custom timeline scanner that only scans spans matching a specific agent name:

``` python
from inspect_scout import (
    Transcript, Result, message_numbering, generate_answer,
    timeline_messages, scan_segments, scanner,
)
from inspect_ai.model import get_model

@scanner
def scan_researcher():
    async def scan(transcript: Transcript) -> list[Result]:
        messages_as_str, extract_references = message_numbering()
        model = get_model()

        async def scan_one(segment):
            prompt = f"Analyze this conversation:\n\n{segment.text}\n\nWhat did the agent find?"
            return await generate_answer(
                prompt, "string", extract_references=extract_references,
            )

        return await scan_segments(
            timeline_messages(
                transcript.timelines[0],
                messages_as_str=messages_as_str,
                model=model,
                include="researcher",  # only spans named "researcher"
                depth=2,
            ),
            scan_one,
        )
    return scan
```

### Parallel Scanning

`scan_segments()` runs a scan function concurrently over a sequence of message segments:

``` python
from inspect_scout import scan_segments

results = await scan_segments(segments, scan_fn)
```

Segments are iterated sequentially (message numbering requires ordered rendering), but each segment's `scan_fn` call runs concurrently. The model's internal connection semaphore handles rate limiting. Results are returned in segment order regardless of completion order.

### Result Reduction

`ResultReducer` provides standard reducers for combining multi-segment results into a single result:

| Reducer | Behavior |
|---------|----------|
| `ResultReducer.any` | `True` if any segment returned `True` |
| `ResultReducer.mean` | Mean of numeric values |
| `ResultReducer.median` | Median of numeric values |
| `ResultReducer.mode` | Most common value |
| `ResultReducer.max` | Maximum numeric value |
| `ResultReducer.min` | Minimum numeric value |
| `ResultReducer.union` | Union of multi-label sets |
| `ResultReducer.last` | Result from the last segment |
| `ResultReducer.llm(model, prompt)` | LLM-based synthesis of results |

Each reducer is an async function with signature `async (list[Result]) -> Result`:

``` python
from inspect_scout import ResultReducer

combined = await ResultReducer.any(results)
```

`ResultReducer.llm()` is a factory that returns a reducer which uses an LLM to synthesize results:

``` python
reducer = ResultReducer.llm(model="openai/gpt-4o", prompt="Synthesize these findings...")
combined = await reducer(results)
```

Default reducers by answer type:

| Answer Type | Default Reducer |
|-------------|----------------|
| `"boolean"` | `ResultReducer.any` |
| `"numeric"` | `ResultReducer.mean` |
| `"string"` | `ResultReducer.last` |
| labels | `ResultReducer.last` |
| `AnswerMultiLabel` | `ResultReducer.union` |
| `AnswerStructured` | `ResultReducer.last` |

When using structured list answers with timeline scanning, reduction is often unnecessary—each span's findings are collected into a combined result set that preserves per-span detail.


## Scanners as Scorers {#scanners-as-scorers}

You may have noticed that scanners are very similar to Inspect [Scorers](https://inspect.aisi.org.uk/scorers.html). This is by design, and it is actually possible to use scanners directly as Inspect scorers.

For example, for the `confusion()` scorer we implemented above:

``` python
@scanner(messages="all")
def confusion() -> Scanner[Transcript]:
    
    async def scan(transcript: Transcript) -> Result:

        # model call eluded for brevity
        output = get_model(...)

        # extract the first word
        match = re.match(r"^\w+", output.completion.strip())

        # return result
        if match:
            answer = match.group(0)
            return Result(
                value=answer.lower() == "yes",
                answer=answer,
                explanation=output.completion,
            )
        else:
            return Result(value=False, explanation=output.completion)

    return scan
```

We can use this directly in an Inspect `Task` as follows:

``` python
from .scanners import confusion

@task
def mytask():
    return Task(
        ...,
        scorer = confusion()
    )
```

We can also use it with the `inspect score` command:

``` bash
inspect score --scorer scanners.py@confusion logfile.eval
```

### Metrics

The metrics used for the scorer will default to `mean()` and `stderr()`---however, you can also explicitly specify metrics on the `@scanner` decorator:

``` python
@scanner(messages="all", metrics=[mean(), bootstrap_stderr()])
def confusion() -> Scanner[Transcript]: ...
```

If you are interfacing with code that expects only `Scorer` instances, you can also use the `as_scorer()` function from Inspect Scout to explicitly convert your scanner to a scorer:

``` python
from inspect_ai import eval
from inspect_scout import as_scorer

from .mytasks import ctf_task
from .scanners import confusion

eval(ctf_task(scorer=as_scorer(confusion())))
```

If your scanner yields [multiple results](#multiple-results) see the discussion above on [Result Sets](#result-sets) for details on how to specify metrics for this case.

## Custom Loaders {#loaders}

When you want to process multiple discrete items from a `Transcript` this might not always fall neatly into single messages or events. For example, you might want to process pairs of user/assistant messages. To do this, create a custom `Loader` that yields the content as required.

For example, here is a `Loader` that yields user/assistant message pairs:

``` python
@loader(messages=["user", "assistant"])
def conversation_turns():
    async def load(
        transcript: Transcript
    ) -> AsyncIterator[list[ChatMessage], None]:
        
        for user,assistant in message_pairs(transcript.messages):
            yield [user, assistant]

    return load
```

Note that just like with scanners, the loader still needs to provide a `messages=["user", "assistant"]` in order to see those messages.

We can now use this loader in a scanner that looks for refusals:

``` python
@scanner(loader=conversation_turns())
def assistant_refusals() -> Scanner[list[ChatMessage]]:

    async def scan(messages: list[ChatMessage]) -> Result:
        user, assistant = messages
        return Result(
            value=is_refusal(assistant.text), 
            explanation=messages_as_str(messages)
        )

    return scan
```