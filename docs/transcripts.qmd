---
title: Transcripts
---

## Overview

Transcripts are the fundamental input to scanners. The `Transcripts` class represents a collection of transcripts that has been selected for scanning, and supports various filtering operations to refine the collection.

## Reading Transcripts

Use the `transcripts_from()` function to read a collection of `Transcripts`:

``` python
from inspect_scout import transcripts_from

# read from a transcript database on S3
transcripts = transcripts_from("s3://weave-rollouts/cybench")

# read from an Inspect log directory
transcripts = transcripts_from("./logs")
```

The `transcripts_from()` function can read transcripts from either:

1.  A transcript database that contains transcripts you have imported from a variety of sources (Agent traces, RL rollouts, Inspect logs, etc.); or

2.  One or more Inspect log directories that contain Inspect `.eval` logs.

See the sections below on the [Transcripts Database] and [Inspect Eval Logs] for additional details on working with each.

## Filtering Transcripts

If you want to scan only a subset of transcripts, you can use the `.where()` method to narrow down the collection. For example:

``` python
from inspect_scout import transcripts_from, metadata as m

transcripts = (
    transcripts_from("./logs")
    .where(m.task_name == "cybench")
    .where(m.model.like("openai/%"))
)
```

See the `Column` documentation for additional details on supported filtering operations.

You can also limit the total number of transcripts as well as shuffle the order of transcripts read (both are useful during scanner development when you don't want to process all transcripts). For example:

``` python
from inspect_scout import transcripts_from, log_metadata as m

transcripts = (
    transcripts_from("./logs")
    .limit(10)
    .shuffle(42)
)
```

## Transcript Fields

{{< include _transcript_fields.md >}}

## Scanning Transcripts

Once you have established your list of transcripts to scan, just pass them to the `scan()` function:

``` python
from inspect_scout import scan, transcripts_from

from .scanners import ctf_environment, java_tool_calls

scan(
    scanners = [ctf_environment(), java_tool_calls()],
    transcripts = transcripts_from("./logs")
)
```

If you want to do transcript filtering and then invoke your scan from the CLI using `scout scan`, then perform the filtering inside a `@scanjob`. For example:

{{< include _scanjob-transcripts.md >}}

## Inspect Eval Logs

The `transcripts_from()` function can read a collection of transcripts directly from an Inspect log directory. You can specify one or more directories and/or individual log files. For example:

``` python
# read from a log directory
transcripts = transcripts_from("./logs")

# read multiple log directories
transcripts = transcripts_from(["./logs", "./logs2"])

# read from one or more log files
transcripts = transcripts_from(
    ["logs/cybench.eval", "logs/swebench.eval"]
)
```

For Inspect logs, the `metadata` field within `TranscriptInfo` includes fields from eval sample metadata. For example:

``` python
transcript.metadata["sample_id"]        # sample uuid 
transcript.metadata["id"]               # dataset sample id 
transcript.metadata["epoch"]            # sample epoch
transcript.metadata["eval_metadata"]    # eval metadata
transcript.metadata["sample_metadata"]  # sample metadata
transcript.metadata["score"]            # main sample score 
transcript.metadata["score_<scorer>"]   # named sample scores
```

See the `LogMetadata` class for details on all of the fields included in `transcript.metadata`. Use `log_metadata` (aliased to `m` below) to do typesafe filtering for Inspect logs:

``` python
from inspect_scout import transcripts_from, log_metadata as m

transcripts = (
    transcripts_from("./logs")
    .where(m.task_name == "cybench")
    .where(m.model.like("openai/%"))
)
```

## Transcripts Database

Scout can analyze transcripts from any source (e.g. Agent traces, RL rollouts, etc.) so long as the transcripts have been organized into a transcripts database. Transcript databases use [Parquet](https://parquet.apache.org) files for storage and can be located in the local filesystem or remote systems like S3.

If you have an existing source of transcript data there are several ways to create a transcript database (`TranscriptDB`):

1.  [Transcript API](#transcript-api): Read and parse transcripts into `Transcript` objects and use the `TranscriptsDB.insert()` function to add them to the database.

2.  [Arrow Import](#arrow-import): Read an existing set of transcripts stored in Arrow/Parquet and pass them to `TranscriptsDB.insert()` as a PyArrow `RecordBatchReader`.

3.  [Parquet Data Lake](#parquet-data-lake): Point the `TranscriptDB` at an existing data lake (ensuring that the records adhere to the transcript database schema).

We'll cover each of these scenarios below. First though, we'll provide a review of the transcript database schema.

### Database Schema

In a transcript database, the only strictly required field is `transcript_id` (although you'll almost always want to also include a `messages` field as that's the main thing targeted by most scanners). 

You can also include `source_*` fields as a reference to where the transcript originated as well as arbitrary other fields (trancript metadata) which are then queryable using the `Transcripts` API.

+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field           | Description                                                                                                                                                                    |
+=================+================================================================================================================================================================================+
| `transcript_id` | Required. A globally unique identifier for a transcript.                                                                                                                       |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `source_type`   | Optional. Type of transcript source (e.g. "weave", "logfire", "eval_log", etc.). Useful for providing a hint to readers about what might be available in the `metadata` field. |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `source_id`     | Optional. Globally unique identifier for a transcript source (e.g. a project id).                                                                                              |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `source_uri`    | Optional. URI for source data (e.g. link to a web page or REST resource for discovering more about the transcript).                                                            |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `messages`      | Optional. List of [ChatMessage](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#messages) with message history.                                                    |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `events`        | Optional. List of [Event](https://inspect.aisi.org.uk/reference/inspect_ai.event.html) with event history (e.g. model events, tool events, etc.)                               |
+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

#### Metadata

You can include arbitrary other fields in your database which will be made available as `Transcript.metadata`. These fields can then be used for filtering in calls to `Transcripts.where()`. Note that `metadata` columns are forwarded into the results database for scans (`transcript_metadata`) so it is generally a good practice to not include large amounts of data in these columns.

#### Messages

The `messages` field is a JSON encoded string of `list[ChatMessage]`. There are several helper functions available within the `inspect_ai` package to assist in converting from the raw message formats of various providers to the Inspect `ChatMessage` format:

+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------+
| Provider API            | Functions                                                                                                                              |
+=========================+========================================================================================================================================+
| OpenAI Chat Completions | [messages_from_openai()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#messages_from_openai)\                            |
|                         | [model_output_from_openai()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#model_output_from_openai)                     |
+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------+
| OpenAI Responses        | [messages_from_openai_responses()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#messages_from_openai_responses)\        |
|                         | [model_output_from_openai_responses()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#model_output_from_openai_responses) |
+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------+
| Anthropic Messages      | [messages_from_anthropic()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#messages_from_anthropic)\                      |
|                         | [model_output_from_anthropic()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#model_output_from_anthropic)               |
+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------+
| Google Generate Content | [messages_from_google()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#messages_from_google)\                            |
|                         | [model_output_from_google()](https://inspect.aisi.org.uk/reference/inspect_ai.model.html#model_output_from_google)                     |
+-------------------------+----------------------------------------------------------------------------------------------------------------------------------------+

The `events` field is less commonly used by scanners so less important to provide. It is a JSON-encded list of Inspect AI events (e.g. `ModelEvent`, `ToolEvxent`, etc.).

Below we'll cover the three main ways to populate a transcript database: the [Transcript API](#transcript-api), [Arrow Import](#arrow-import), and using an existing  [Parquet Data Lake](#parquet-data-lake).

### Transcript API

To create a transcripts database, use the `transcripts_db()` function to get a `TranscriptsDB` interface and then insert `Transcript` objects. In this example imagine we have a `read_weave_transcripts()` function which can read transcripts from an external JSON transcript format:

``` python
from inspect_scout import transcripts_db

from .readers import read_json_transcripts

# create/open database
async with transcripts_db("s3://my-transcripts") as db:

    # read transcripts to insert
    transcripts = read_json_transcripts()

    # insert into database
    await db.insert(transcripts)
```

Once you've created a database and populated it with transcripts, you can read from it using `transcripts_from()`:

``` python
from inspect_scout import scan, transcripts_from

scan(
    scanners=[...],
    transcripts=transcripts_from("s3://my-transcripts")
)
```

#### Streaming

Each call to `db.insert()` will minimally create one Parquet file, but will break transcripts across multiple files as required, limiting the size of files to 100MB. This will create a storage layout optimized for fast queries and content reading. Consequently, when importing a large number of transcripts you should always write a generator to yield transcripts rather than making many calls to `db.insert()` (which is likely to result in more Parquet files than is ideal).

For example, we might implement `read_json_transcripts()` like this:

``` python
from pathlib import Path
from typing import AsyncIterator
from inspect_scout import Transcript

async def read_json_transcripts(dir: Path) -> AsyncIterator[Transcript]:
    json_files = list(dir.rglob("*.json"))
    for json_file in json_files:
        yield await json_to_transcript(json_file)

async def json_to_transcript(json_file: Path) -> Transcript:
    # convert json_file to Transcript
    return Transcript(...)
```

We can then pass this generator function directly to `db.insert()`:

``` python
async with transcripts_db("s3://my-transcripts") as db:
    await db.insert(read_json_transcripts())
```

Note that transcript insertion is idempotent---once a transcript with a given ID has been inserted it will not be inserted again. This means that you can safely resume imports that are interrupted, and only new transcripts will be added.

#### Transcripts

Here is how we might implement `json_to_transcript()`:

``` python
from pathlib import Path
from typing import AsyncIterator
from inspect_ai.model import (
    messages_from_openai, model_output_from_openai
)
from inspect_scout import Transcript

async def json_to_transcript(json_file: Path) -> Transcript:
    with open(json_file, "r") as f:
        json_data: dict[str,Any] = json.loads(f.read())
        return Transcript(
            transcript_id = json_data["trace_id"],
            source_type="abracadabra",
            source_id=json_data["project_id"],
            metadata=json_data["attributes"],
            messages=await json_to_messages(
                input=json_data["inputs"]["messages"], 
                output=json_data["output"]
            )
        )
    
# convert raw model input and output to inspect messages
async def json_to_messages(
    input: list[dict[str, Any]], output: dict[str, Any]
) -> list[ChatMessage]:
    # start with input messages
    messages = await messages_from_openai(input)

    # extract and append assistant message from output
    output = await model_output_from_openai(output)
    messages.append(output.message)

    # return full message history for transcript
    return messages
```

Note that we use the `messages_from_openai()` and `model_output_from_openai()` function from `inspect_ai` to convert the raw model payloads in the trace data to the correct types for the transcript database.

The most important fields to populate are `transcript_id` and `messages`. The `source_*` fields are also useful for providing additional context. The `metadata` field, while not required, is a convenient way to provide additional transcript attributes which may be useful for filtering or analysis. The `events` field is not required and useful primarily for more complex multi-agent transcripts.


### Arrow Import

In some cases you may already have arrow-accessible data (e.g. from Parquet files or a database that supports yielding arrow batches) that you want to insert directly into a transcript database. So long as your data conforms to the [schema](#database-schema) described above, you can do this by passing a PyArrow `RecordBatchReader` to `db.insert()`.

For example, to read from existing Parquet files using the PyArrow dataset API:

```python
import pyarrow.dataset as ds
from inspect_scout import transcripts_db

# read from existing parquet files
dataset = ds.dataset("path/to/parquet/files", format="parquet")
reader = dataset.scanner().to_reader()

async with transcripts_db("s3://my-transcripts") as db:
    await db.insert(reader)
```

You can also use DuckDB to query and transform data before import:

```python
import duckdb
from inspect_scout import transcripts_db

conn = duckdb.connect("my_database.db")
reader = conn.execute("""
    SELECT
        trace_id as transcript_id,
        messages,
        'myapp' as source_type,
        project_id as source_id
    FROM traces
""").fetch_record_batch()

async with transcripts_db("s3://my-transcripts") as db:
    await db.insert(reader)
```

### Parquet Data Lake

If you have transcripts already stored in Parquet format you don't need to use `db.insert()` at all. So long as your Parquet files conform to the [transcript database schema](#database-schema) described above then you can read them directly using `transcripts_from()`. For example:

```python
from inspect_scout import transcripts_from

# read from an existing parquet data lake
transcripts = transcripts_from(
    "s3://my-transcripts-data-lake/cyber"
)
```

### Importing Logs

If you prefer to keep all of your transcripts (including ones from Inspect evals) in a transcript database, you can easily import Inspect logs as follows:

``` python
from inspect_scout import transcripts_db, transcripts_from

async with transcripts_db("s3://my-transcript-db/") as db:
    await db.insert(transcripts_from("./logs"))
```

You could also insert a filtered list of transcripts:

``` python
async with transcripts_db("s3://my-transcript-db/") as db:

    transcripts = (
        transcripts_from("./logs")
        .where(m.task_name == "cybench")
        .where(m.model.like("openai/%"))
    )

    await db.insert(transcripts)
```