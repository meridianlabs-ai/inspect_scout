[
  {
    "objectID": "reference/scanner.html",
    "href": "reference/scanner.html",
    "title": "Scanner API",
    "section": "",
    "text": "Scan transcript content.\n\nSource\n\nclass Scanner(Protocol[T]):\n    def __call__(self, input: T, /) -&gt; Awaitable[Result]\n\ninput T\n\nInput to scan.\n\n\n\n\n\nUnion of all valid scanner input types.\n\nSource\n\nScannerInput = Union[\n    Transcript,\n    ChatMessage,\n    Sequence[ChatMessage],\n    Event,\n    Sequence[Event],\n]\n\n\n\nScan result.\n\nSource\n\nclass Result(BaseModel)\n\n\n\nvalue JsonValue\n\nScan value (can be None if the scan didn’t find what is was looking for).\n\nexplanation str | None\n\nExplanation of result (optional).\n\nmetadata dict[str, Any] | None\n\nAdditional metadata related to the result (optional)\n\nreferences list[Reference]\n\nReferences to relevant messages or events.\n\n\n\n\n\n\nReference to scanned content.\n\nSource\n\nclass Reference(BaseModel)\n\n\n\ntype Literal['message', 'event']\n\nReference type.\n\nid str\n\nReference id (message or event id)\n\n\n\n\n\n\nScan error (runtime error which occurred during scan).\n\nSource\n\nclass Error(BaseModel)\n\n\n\ntranscript_id str\n\nTarget transcript id.\n\nscanner str\n\nScanner name.\n\nerror str\n\nError message.\n\ntraceback str\n\nError traceback.\n\n\n\n\n\n\nLoad transcript data.\n\nSource\n\nclass Loader(Protocol[TLoaderResult]):\n    def __call__(\n        self,\n        transcript: Transcript,\n    ) -&gt; AsyncIterator[TLoaderResult]\n\ntranscript Transcript\n\nTranscript to yield from.",
    "crumbs": [
      "Reference",
      "Python API",
      "scanner"
    ]
  },
  {
    "objectID": "reference/scanner.html#scanner",
    "href": "reference/scanner.html#scanner",
    "title": "Scanner API",
    "section": "",
    "text": "Scan transcript content.\n\nSource\n\nclass Scanner(Protocol[T]):\n    def __call__(self, input: T, /) -&gt; Awaitable[Result]\n\ninput T\n\nInput to scan.\n\n\n\n\n\nUnion of all valid scanner input types.\n\nSource\n\nScannerInput = Union[\n    Transcript,\n    ChatMessage,\n    Sequence[ChatMessage],\n    Event,\n    Sequence[Event],\n]\n\n\n\nScan result.\n\nSource\n\nclass Result(BaseModel)\n\n\n\nvalue JsonValue\n\nScan value (can be None if the scan didn’t find what is was looking for).\n\nexplanation str | None\n\nExplanation of result (optional).\n\nmetadata dict[str, Any] | None\n\nAdditional metadata related to the result (optional)\n\nreferences list[Reference]\n\nReferences to relevant messages or events.\n\n\n\n\n\n\nReference to scanned content.\n\nSource\n\nclass Reference(BaseModel)\n\n\n\ntype Literal['message', 'event']\n\nReference type.\n\nid str\n\nReference id (message or event id)\n\n\n\n\n\n\nScan error (runtime error which occurred during scan).\n\nSource\n\nclass Error(BaseModel)\n\n\n\ntranscript_id str\n\nTarget transcript id.\n\nscanner str\n\nScanner name.\n\nerror str\n\nError message.\n\ntraceback str\n\nError traceback.\n\n\n\n\n\n\nLoad transcript data.\n\nSource\n\nclass Loader(Protocol[TLoaderResult]):\n    def __call__(\n        self,\n        transcript: Transcript,\n    ) -&gt; AsyncIterator[TLoaderResult]\n\ntranscript Transcript\n\nTranscript to yield from.",
    "crumbs": [
      "Reference",
      "Python API",
      "scanner"
    ]
  },
  {
    "objectID": "reference/scanner.html#utils",
    "href": "reference/scanner.html#utils",
    "title": "Scanner API",
    "section": "Utils",
    "text": "Utils\n\nmessages_as_str\nConcatenate list of chat messages into a string.\n\nSource\n\ndef messages_as_str(messages: list[ChatMessage]) -&gt; str\n\nmessages list[ChatMessage]\n\nList of chat messages",
    "crumbs": [
      "Reference",
      "Python API",
      "scanner"
    ]
  },
  {
    "objectID": "reference/scanner.html#types",
    "href": "reference/scanner.html#types",
    "title": "Scanner API",
    "section": "Types",
    "text": "Types\n\nMessageType\nMessage types.\n\nSource\n\nMessageType = Literal[\"system\", \"user\", \"assistant\", \"tool\"]\n\n\nEventType\nEvent types.\n\nSource\n\nEventType = Literal[\n    \"model\",\n    \"tool\",\n    \"approval\",\n    \"sandbox\",\n    \"info\",\n    \"logger\",\n    \"error\",\n    \"span_begin\",\n    \"span_end\",\n]",
    "crumbs": [
      "Reference",
      "Python API",
      "scanner"
    ]
  },
  {
    "objectID": "reference/scanner.html#decorators",
    "href": "reference/scanner.html#decorators",
    "title": "Scanner API",
    "section": "Decorators",
    "text": "Decorators\n\nscanner\nDecorator for registering scanners.\n\nSource\n\ndef scanner(\n    factory: ScannerFactory[P, T] | None = None,\n    *,\n    loader: Loader[T] | None = None,\n    messages: list[MessageType] | Literal[\"all\"] | None = None,\n    events: list[EventType] | Literal[\"all\"] | None = None,\n    name: str | None = None,\n) -&gt; (\n    ScannerFactory[P, T]\n    | Callable[[ScannerFactory[P, T]], ScannerFactory[P, T]]\n    | Callable[[ScannerFactory[P, TM]], ScannerFactory[P, ScannerInput]]\n    | Callable[[ScannerFactory[P, TE]], ScannerFactory[P, ScannerInput]]\n)\n\nfactory ScannerFactory[P, T] | None\n\nDecorated scanner function.\n\nloader Loader[T] | None\n\nCustom data loader for scanner.\n\nmessages list[MessageType] | Literal['all'] | None\n\nMessage types to scan.\n\nevents list[EventType] | Literal['all'] | None\n\nEvent types to scan.\n\nname str | None\n\nScanner name (defaults to function name).\n\n\n\n\nloader\nDecorator for registering loaders.\n\nSource\n\ndef loader(\n    *,\n    name: str | None = None,\n    messages: list[MessageType] | Literal[\"all\"] | None = None,\n    events: list[EventType] | Literal[\"all\"] | None = None,\n    content: TranscriptContent | None = None,\n) -&gt; Callable[[LoaderFactory[P, TLoaderResult]], LoaderFactory[P, TLoaderResult]]\n\nname str | None\n\nLoader name (defaults to function name).\n\nmessages list[MessageType] | Literal['all'] | None\n\nMessage types to load from.\n\nevents list[EventType] | Literal['all'] | None\n\nEvent types to load from.\n\ncontent TranscriptContent | None\n\nTranscript content filter.",
    "crumbs": [
      "Reference",
      "Python API",
      "scanner"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "Python API\n\n\n\n\n\n\n\nScanning\nScan transcripts and manage scan jobs.\n\n\nResults\nStatus and results of scan jobs.\n\n\nTranscripts\nRead and filter transcripts.\n\n\nScanners\nImplement scanners and loaders.\n\n\nAsync\nAsync functions for scanning.\n\n\n\n\n\nScount CLI\n\n\n\n\n\n\n\nscout scan\nScan transcripts.\n\n\nscout scan resume\nResume a scan which is incomplete due to interruption or errors.\n\n\nscout scan complete\nComplete a scan which is incomplete due to errors (errors are not retried).\n\n\nscout scan list\nList the scans within a scan results directory.\n\n\nscout scan status\nPrint the status of a scan.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "reference/scout_scan.html",
    "href": "reference/scout_scan.html",
    "title": "scout scan",
    "section": "",
    "text": "Scan transcripts and read results.\nPass a FILE which is either a Python script that contains @scanner or @scanjob decorated functions or a config file (YAML or JSON) that adheres to the ScanJobConfig schema.",
    "crumbs": [
      "Reference",
      "Scout CLI"
    ]
  },
  {
    "objectID": "reference/scout_scan.html#scout-scan-complete",
    "href": "reference/scout_scan.html#scout-scan-complete",
    "title": "scout scan",
    "section": "scout scan complete",
    "text": "scout scan complete\nComplete a scan which is incomplete due to errors (errors are not retried).\n\nUsage\nscout scan complete [OPTIONS] SCAN_LOCATION\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--display\nchoice (rich | plain | none)\nSet the display type (defaults to ‘rich’)\nrich\n\n\n--log-level\nchoice (debug | trace | http | info | warning | error | critical | notset)\nSet the log level (defaults to ‘warning’)\nwarning\n\n\n--debug\nboolean\nWait to attach debugger\nFalse\n\n\n--debug-port\ninteger\nPort number for debugger\n5678\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI"
    ]
  },
  {
    "objectID": "reference/scout_scan.html#scout-scan-list",
    "href": "reference/scout_scan.html#scout-scan-list",
    "title": "scout scan",
    "section": "scout scan list",
    "text": "scout scan list\nList the scans within the scans dir.\n\nUsage\nscout scan list [OPTIONS] [SCANS_DIR]\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--display\nchoice (rich | plain | none)\nSet the display type (defaults to ‘rich’)\nrich\n\n\n--log-level\nchoice (debug | trace | http | info | warning | error | critical | notset)\nSet the log level (defaults to ‘warning’)\nwarning\n\n\n--debug\nboolean\nWait to attach debugger\nFalse\n\n\n--debug-port\ninteger\nPort number for debugger\n5678\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI"
    ]
  },
  {
    "objectID": "reference/scout_scan.html#scout-scan-resume",
    "href": "reference/scout_scan.html#scout-scan-resume",
    "title": "scout scan",
    "section": "scout scan resume",
    "text": "scout scan resume\nResume a scan which is incomplete due to interruption or errors (errors are retried).\n\nUsage\nscout scan resume [OPTIONS] SCAN_LOCATION\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--display\nchoice (rich | plain | none)\nSet the display type (defaults to ‘rich’)\nrich\n\n\n--log-level\nchoice (debug | trace | http | info | warning | error | critical | notset)\nSet the log level (defaults to ‘warning’)\nwarning\n\n\n--debug\nboolean\nWait to attach debugger\nFalse\n\n\n--debug-port\ninteger\nPort number for debugger\n5678\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI"
    ]
  },
  {
    "objectID": "reference/scout_trace.html",
    "href": "reference/scout_trace.html",
    "title": "scout trace",
    "section": "",
    "text": "List and read execution traces.\nInspect Scout includes a TRACE log-level which is right below the HTTP and INFO log levels (so not written to the console by default). However, TRACE logs are always recorded to a separate file, and the last 10 TRACE logs are preserved. The ‘trace’ command provides ways to list and read these traces.",
    "crumbs": [
      "Reference",
      "Scout CLI",
      "scout trace"
    ]
  },
  {
    "objectID": "reference/scout_trace.html#scout-trace-list",
    "href": "reference/scout_trace.html#scout-trace-list",
    "title": "scout trace",
    "section": "scout trace list",
    "text": "scout trace list\nList all trace files.\n\nUsage\nscout trace list [OPTIONS]\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--json\nboolean\nOutput listing as JSON\nFalse\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI",
      "scout trace"
    ]
  },
  {
    "objectID": "reference/scout_trace.html#scout-trace-dump",
    "href": "reference/scout_trace.html#scout-trace-dump",
    "title": "scout trace",
    "section": "scout trace dump",
    "text": "scout trace dump\nDump a trace file to stdout (as a JSON array of log records).\n\nUsage\nscout trace dump [OPTIONS] [TRACE_FILE]\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--filter\ntext\nFilter (applied to trace message field).\n\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI",
      "scout trace"
    ]
  },
  {
    "objectID": "reference/scout_trace.html#scout-trace-http",
    "href": "reference/scout_trace.html#scout-trace-http",
    "title": "scout trace",
    "section": "scout trace http",
    "text": "scout trace http\nView all HTTP requests in the trace log.\n\nUsage\nscout trace http [OPTIONS] [TRACE_FILE]\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--filter\ntext\nFilter (applied to trace message field).\n\n\n\n--failed\nboolean\nShow only failed HTTP requests (non-200 status)\nFalse\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI",
      "scout trace"
    ]
  },
  {
    "objectID": "reference/scout_trace.html#scout-trace-anomalies",
    "href": "reference/scout_trace.html#scout-trace-anomalies",
    "title": "scout trace",
    "section": "scout trace anomalies",
    "text": "scout trace anomalies\nLook for anomalies in a trace file (never completed or cancelled actions).\n\nUsage\nscout trace anomalies [OPTIONS] [TRACE_FILE]\n\n\nOptions\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n--filter\ntext\nFilter (applied to trace message field).\n\n\n\n--all\nboolean\nShow all anomolies including errors and timeouts (by default only still running and cancelled actions are shown).\nFalse\n\n\n--help\nboolean\nShow this message and exit.\nFalse\n\n\n\n\n\nSubcommands",
    "crumbs": [
      "Reference",
      "Scout CLI",
      "scout trace"
    ]
  },
  {
    "objectID": "transcripts.html",
    "href": "transcripts.html",
    "title": "Transcripts",
    "section": "",
    "text": "Transcripts are the fundamental input to scanners, and are read from one or more Inspect logs. The Transcripts class represents a collection of transcripts that has been selected for scanning. This is an index of TranscriptInfo rather than full transcript content, and supports various filtering operations to refine the collection.",
    "crumbs": [
      "Getting Started",
      "Using Scout"
    ]
  },
  {
    "objectID": "transcripts.html#overview",
    "href": "transcripts.html#overview",
    "title": "Transcripts",
    "section": "",
    "text": "Transcripts are the fundamental input to scanners, and are read from one or more Inspect logs. The Transcripts class represents a collection of transcripts that has been selected for scanning. This is an index of TranscriptInfo rather than full transcript content, and supports various filtering operations to refine the collection.",
    "crumbs": [
      "Getting Started",
      "Using Scout"
    ]
  },
  {
    "objectID": "transcripts.html#reading-transcripts",
    "href": "transcripts.html#reading-transcripts",
    "title": "Transcripts",
    "section": "Reading Transcripts",
    "text": "Reading Transcripts\nUse the transcripts_from_logs() function to read a collection of Transcripts from one or more Inspect logs:\nfrom inspect_scout import transcripts_from_logs\n\n# read from a log directory\ntranscripts = transcripts_from_logs(\"./logs\")\n\n# read from an S3 log directory\ntranscripts = transcripts_from_logs(\"s3://my-inspect-logs\")\n\n# read multiple log directories\ntranscripts = transcripts_from_logs([\"./logs\", \"./logs2\"])\n\n# read from one or more log files\ntranscripts = transcripts_from_logs(\n    [\"logs/cybench.eval\", \"logs/swebench.eval\"]\n)\nYou can also specify transcripts using an evals_df() or samples_df() read using the Inspect log data frame functions:\nfrom inspect_ai.analysis import samples_df\nfrom inspect_scout import transcripts_from_logs\n\n# read samples, filter as required, etc.\nsamples = samples_df(\"./logs\")\n\n# transcripts\ntranscripts = transcripts_from_logs(samples)",
    "crumbs": [
      "Getting Started",
      "Using Scout"
    ]
  },
  {
    "objectID": "transcripts.html#filtering-transcripts",
    "href": "transcripts.html#filtering-transcripts",
    "title": "Transcripts",
    "section": "Filtering Transcripts",
    "text": "Filtering Transcripts\nIf you want to scan only a subset of transcripts, you can use the .where() method to narrow down the collection. For example:\nfrom inspect_scout import transcripts_from_logs, log_metadata as m\n\ntranscripts = (\n    transcripts_from_logs(\"./logs\")\n    .where(m.task_name == \"cybench\")\n    .where(m.model.like(\"openai/%\"))\n)\nSee the Column documentation for additional details on supported filtering operations.\nSee the LogMetadata documentation for the standard metadata fields that are exposed from logs for filtering.\nYou can also limit the total number of transcripts as well as shuffle the order of transcripts read (both are useful during scanner development when you don’t want to process all transcripts). For example:\nfrom inspect_scout import transcripts_from_logs, log_metadata as m\n\ntranscripts = (\n    transcripts_from_logs(\"./logs\")\n    .limit(10)\n    .shuffle(42)\n)",
    "crumbs": [
      "Getting Started",
      "Using Scout"
    ]
  },
  {
    "objectID": "transcripts.html#scanning-transcripts",
    "href": "transcripts.html#scanning-transcripts",
    "title": "Transcripts",
    "section": "Scanning Transcripts",
    "text": "Scanning Transcripts\nOnce you have established your list of transcripts to scan, just pass them to the scan() function:\nfrom inspect_scout import scan, transcripts_from_logs\n\nfrom .scanners import ctf_environment, java_tool_calls\n\nscan(\n    scanners = [ctf_environment(), java_tool_calls()],\n    transcripts = transcripts_from_logs(\"./logs\")\n)\nIf you want to do transcript filtering and then invoke your scan from the CLI using scout scan, then perform the filtering inside a @scanjob. For example:\n\n\ncybench_scan.py\n\nfrom inspect_scout (\n    import ScanJob, scanjob, transcripts_from_logs, log_metadata as m\n)\n\nfrom .scanners import deception, tool_errors\n\n@scanjob\ndef cybench_job(logs: str = \"./logs\") -&gt; ScanJob:\n\n    transcripts = transcripts_from_logs(logs)\n    transcripts = transcripts.where(m.task_name == \"cybench\")\n\n    return ScanJob(\n        scanners = [deception(), java_tool_usages()],\n        transcripts = transcripts\n    )\n\nThen from the CLI:\nscout scan cybench.py -S logs=./logs --model openai/gpt-5\nThe -S argument enables you to pass arguments to the @scanjob function (in this case determining what directory to read logs from).",
    "crumbs": [
      "Getting Started",
      "Using Scout"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Scout",
    "section": "",
    "text": "Welcome to Inspect Scout, a tool for in-depth analysis of Inspect AI transcripts. Scout has the following core features:\n\nScan full sample transcripts or individual messages or events.\nHigh performance parallel processing of transcript content.\nResume scans that are stopped due to errors or interruptions.\nTightly integrated with Inspect data frames for input and analysis.\n\n\n\nInstall the inspect_scout package from GitHub as follows:\npip install git+https://github.com/meridianlabs-ai/inspect_scout\nInspect Scout also depends on the development version of Inspect AI, which will be installed automatically when you install Scout.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Inspect Scout",
    "section": "",
    "text": "Welcome to Inspect Scout, a tool for in-depth analysis of Inspect AI transcripts. Scout has the following core features:\n\nScan full sample transcripts or individual messages or events.\nHigh performance parallel processing of transcript content.\nResume scans that are stopped due to errors or interruptions.\nTightly integrated with Inspect data frames for input and analysis.\n\n\n\nInstall the inspect_scout package from GitHub as follows:\npip install git+https://github.com/meridianlabs-ai/inspect_scout\nInspect Scout also depends on the development version of Inspect AI, which will be installed automatically when you install Scout.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#scout-basics",
    "href": "index.html#scout-basics",
    "title": "Inspect Scout",
    "section": "Scout Basics",
    "text": "Scout Basics\nBelow we’ll provide some simple examples of creating and using Scout scanners. Some core components we’ll make use of here include:\n\nTranscript — LLM conversation to analyze (e.g. an Inspect sample trajectory).\nScanner — Function that takes an input from a Transcript and returns a Result (conceptually very similar to an Inspect Scorer).\nResults — Data frame(s) that includes the results of scanners applied to transcripts.\n\n\nCreating a Scanner\nThe following illustrates the implementation of a simple LLM scanner (click on the numbers at right for further explanation):\n\n\nscanner.py\n\nfrom inspect_scout import (\n    Result, Scanner, Transcript, scanner, messages_as_str\n)\n\nfrom inspect_ai.model import get_model\n\n1@scanner(messages=\"all\")\ndef ctf_environment() -&gt; Scanner[Transcript]:\n    \n    async def scan(transcript: Transcript) -&gt; Result:\n\n        # call model\n2        output = await get_model().generate(\n            \"Here is a transcript of an LLM agent \" +\n            \"solving a cybersecurity challenge:\\n\\n\" +\n3            \"===================================\" +\n            messages_as_str(transcript.messages) +\n            \"===================================\\n\\n\" +\n            \"In the transcript above do you detect any \" +\n            \"instances of environment misconfiguration \" +\n            \"preventing the agent from completing it's \" + \n            \"task? If no, simply respond with 'No'. \" +\n            \"If yes, respond with 'Yes' followed by an \" +\n            \"explanation.\"\n        )\n\n        # return result (value + full model completion)\n4        return Result(\n            value=output.completion.lower().startswith(\"yes\"),\n            explanation=result.completion\n        )\n\n    return scan\n\n\n1\n\nScanners are decorated with @scanner so they can specify the exact subset of content they need to read. In this case only messages (and not events) will be read from the log, decreasing load time.\n\n2\n\nScanners frequently use models to perform scanning. Calling get_model() utilizes the default model for the scan job (which can be specified in the top level call to scan).\n\n3\n\nConvert the message history into a string for presentation to the model.\n\n4\n\nAs with scorers, results also include additional context (here the full model completion).\n\n\n\n\nRunning a Scan\nWe can now run that scanner on our log files. The Scanner will be called once for each sample trajectory in the log (total samples * epochs):\nscout scan scanner.py --transcripts ./logs -model openai/gpt-5\n\n\nAdding a Scanner\nLet’s add another scanner that looks for uses of Java in tool calls:\n@scanner(events=[\"tool\"]) \ndef java_tool_usages() -&gt; Scanner[ToolEvent]:\n    \n    async def scan(event: ToolEvent) -&gt; Result:\n        if \"java\" in str(event.arguments).lower():\n            return Result(\n                value=True, \n                explanation=str(event.arguments)\n            )\n        else:\n            return Result(value=False)\n       \n    return scan\nNote that we specify events=[\"tool\"] to constrain reading to only tool events, and that our function takes an individual event rather than a Transcript.\nIf you add this scanner to the same source file as the ctf_environment() scanner then scout scan will run both of the scanners using the same scout scan scanner.py command,\n\n\nScan Jobs\nYou may want to import scanners from other modules and compose them into a ScanJob. To do this, add a @scanjob decorated function to your source file (it will be used in preference to @scanner decorated functions).\nA ScanJob can also include transcripts or any other option that you can pass to scout scan (e.g. model). For example:\n\n\nscanner.py\n\nfrom inspect_scout import ScanJob, scanjob\n\n@scanjob\ndef job() -&gt; ScanJob:\n    return ScanJob(\n        scanners=[ctf_environment(), java_tool_usages()],\n        transcripts=\"./logs\",\n        model=\"openai/gpt-5\"\n    )\n\nYou can then use the same command to run the job (scout scan will prefer a @scanjob defined in a file to individual scanners):\nscout scan scanner.py\nYou can also specify a scan job using YAML or JSON. For example, the following is equivalent to the example above:\n\n\nscan.yaml\n\nscanners:\n  - name: deception\n    file: scanner.py\n  - name: java_tool_usages\n    file: scanner.py\n\ntranscripts: logs\n\nmodel: openai/gpt-5\n\nWhich can be executed with:\nscout scan scan.yaml\n\n\nScan Results\nBy default, the results of scans are written into the ./scans directory. You can override this using the --results option—both file paths and S3 buckets are supported.\nEach scan is stored in its own directory and has both metadata about the scan (configuration, errors, summary of results) as well as parquet files that contain the results. You can read the results either as a dict of Pandas data frames or as a DuckDB database (there will be a table for each scanner).\n# results as pandas data frames\nresults = scan_results(\"scans/scan_id=iGEYSF6N7J3AoxzQmGgrZs\")\ndeception_df = results.scanners[\"deception\"]\ntool_errors_df = results.scanners[\"tool_errors\"]\n\n# results as duckdb database \nresults = scan_results_db(\"scans/scan_id=iGEYSF6N7J3AoxzQmGgrZs\")\nwith results:\n    # run queries to read data frames\n    df = results.conn.execute(\"SELECT ...\").fetch_df()\n\n    # export entire database as file\n    results.to_file(\"results.duckdb\")\n\n\nHandling Errors\nIf a scan job is interrupted either due to cancellation (Ctrl+C) or a runtime error, you can resume the scan from where it left off using the scan resume command. For example:\nscout scan resume \"scans/scan_id=iGEYSF6N7J3AoxzQmGgrZs\"\nIf errors occur during an individual scan, they are caught and reported. You can then either retry the failed scans with scan resume or complete the scan (ignoring errors) with scan complete:",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#transcripts",
    "href": "index.html#transcripts",
    "title": "Inspect Scout",
    "section": "Transcripts",
    "text": "Transcripts\nIn the example(s) above we scanned the samples from a single Inspect log file. More commonly though you’ll target an entire log directory or a subset of logs in that directory. For example, here we scan all of Cybench logs in the ./logs directory:\nfrom inspect_scout (\n    import scan, transcripts_from_logs, log_metadata as m\n)\n\nfrom .scanners import deception, tool_errors\n\ntranscripts = transcripts_from_logs(\"./logs\")\ntranscripts = transcripts.where(m.task_name == \"cybench\")\n\nstatus = scan(\n    scanners = [ctf_environment(), tool_errors()],\n    transcripts = transcripts\n)\nThe log_metadata object (aliased to m) provides a typed way to specified where() clauses for filtering transcripts.\nNote that doing this query required us to switch to the Python scan() API. We can still use the CLI if we wrap our transcript query in a ScanJob:\n\n\ncybench_scan.py\n\nfrom inspect_scout (\n    import ScanJob, scanjob, transcripts_from_logs, log_metadata as m\n)\n\nfrom .scanners import deception, tool_errors\n\n@scanjob\ndef cybench_job(logs: str = \"./logs\") -&gt; ScanJob:\n\n    transcripts = transcripts_from_logs(logs)\n    transcripts = transcripts.where(m.task_name == \"cybench\")\n\n    return ScanJob(\n        scanners = [deception(), java_tool_usages()],\n        transcripts = transcripts\n    )\n\nThen from the CLI:\nscout scan cybench.py -S logs=./logs --model openai/gpt-5\nThe -S argument enables you to pass arguments to the @scanjob function (in this case determining what directory to read logs from).",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#parallelism",
    "href": "index.html#parallelism",
    "title": "Inspect Scout",
    "section": "Parallelism",
    "text": "Parallelism\nThe Scout scanning pipeline is optimized for parallel reading and scanning as well as minimal memory consumption. There are a few options you can use to tune parallelism:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n--max-transcripts\nThe maximum number of transcripts to scan in parallel (defaults to 25). You can set this higher if your model API endpoint can handle larger numbers of concurrent requests.\n\n\n--max-connections\nThe maximum number of concurrent requests to the model provider (defaults to --max-transcripts).\n\n\n--max-processes\nThe maximum number of processes to use for parsing and scanning (defaults to the number of CPUs on the system).",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#learning-more",
    "href": "index.html#learning-more",
    "title": "Inspect Scout",
    "section": "Learning More",
    "text": "Learning More\nSee the following articles to learn more about using Scout:\n\nTranscripts: Reading and filtering transcripts for scanning.\nScanners: Implementing custom scanners and loaders.\nResults: Collecting and analyzing scanner results.\nReference: Detailed documentation on the Scout Python API and CLI commands.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The results of scans are stored in directory on the local filesystem (by default ./scans) or in a remote S3 bucket. When a scan job is completed its directory is printed, and you can also use the scan_list() function or scout scan list command to enumerate scan jobs.\nScan results include the following:\n\nScan configuration (e.g. options passed to scan() or to scout scan).\nTranscripts scanned and scanners executed and errors which occurred during the last scan.\nA set of Parquet files with scan results (one for each scanner). There are functions available to interface with these files as Pandas data frames or DuckDB databases.",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Results"
    ]
  },
  {
    "objectID": "results.html#overview",
    "href": "results.html#overview",
    "title": "Results",
    "section": "",
    "text": "The results of scans are stored in directory on the local filesystem (by default ./scans) or in a remote S3 bucket. When a scan job is completed its directory is printed, and you can also use the scan_list() function or scout scan list command to enumerate scan jobs.\nScan results include the following:\n\nScan configuration (e.g. options passed to scan() or to scout scan).\nTranscripts scanned and scanners executed and errors which occurred during the last scan.\nA set of Parquet files with scan results (one for each scanner). There are functions available to interface with these files as Pandas data frames or DuckDB databases.",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Results"
    ]
  },
  {
    "objectID": "results.html#workflow",
    "href": "results.html#workflow",
    "title": "Results",
    "section": "Workflow",
    "text": "Workflow\n\nScout CLI\nThe scout scan command will print its status at the end of its run. If all of the scanners completed without errors you’ll see a message indicating the scan is complete along with a pointer to the scan directory where results are stored:\n\nYou can then pass that directory to the scan_results() function to get access to the underlying data frames for each scanner:\nfrom inspect_scout import scan_results\n\nresults = scan_results(\"scans/scan_id=3ibJe9cg7eM5zo3h5Hpbr8\")\ndeception_df = results.scanners[\"deception\"]\ntool_errors_df = results.scanners[\"tool_errors\"]\n\n\nPython API\nThe scan() function returns a Status object which indicates whether the scan completed successfully (in which case the scanner results are available for analysis). You’ll therefore want to check the .completed field before proceeding to read the results. For example:\nfrom inspect_scout import (\n    scan, scan_results, transcripts_from_logs\n)\n\nfrom .scanners import ctf_environment, java_tool_calls\n\nstatus = scan(\n    transcripts=transcripts_from_logs(\"./logs\"),\n    scanners=[ctf_environment(), java_tool_calls()]\n)\n\nif status.complete:\n    results = scan_results(status.location)\n    deception_df = results.scanners[\"deception\"]\n    tool_errors_df = results.scanners[\"tool_errors\"]\n\n\nDuckDB\nThe above examples demonstrated reading scanner output as Pandas data frames. If you prefer, you can also read scanner data from a DuckDB database as follows:\nresults = scan_results_db(\"scans/scan_id=3ibJe9cg7eM5zo3h5Hpbr8\")\nwith results:\n    # run queries to read data frames\n    df = results.conn.execute(\"SELECT ...\").fetch_df()\n\n    # export entire database as file\n    results.to_file(\"results.duckdb\")",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Results"
    ]
  },
  {
    "objectID": "results.html#results-data",
    "href": "results.html#results-data",
    "title": "Results",
    "section": "Results Data",
    "text": "Results Data\nThe Results object returned from scan_results() includes both metadata about the scan as well as the scanner data frames:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\ncomplete\nbool\nIs the job complete? (all transcripts scanned)\n\n\nspec\nScanSpec\nScan specification (transcripts, scanners, options, etc.)\n\n\nlocation\nstr\nLocation of scan directory\n\n\nsummary\nSummary\nSummary of scan (results, errors, tokens, etc.)\n\n\nerrors\nlist[Error]\nErrors during last scan attempt.\n\n\nscanners\ndict[str, pd.DataFrame]\nResults data for each scanner (see Data Frames for details)\n\n\n\n\nData Frames\n\nThe data frames available for each scanner contain information about the source evaluation and transcript, the results found for each transcript, as well as data on token usage, model calls, and errors which may have occurred during the scan.\nThe data frame includes the following fields (note that some fields included embedded JSON data, these are all noted below):\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\ntranscript_id\nstr\nGlobally unique identifier for a transcript (maps to EvalSample.uuid in the Inspect log or sample_id in Inspect analysis data frames).\n\n\ntranscript_source_id\nstr\nGlobally unique identifier for a transcript source (maps to `eval_id` in the Inspect log and analysis data frames).\n\n\ntranscript_source_uri\nstr\nURI for source data (e.g. full path to the Inspect log file).\n\n\ntranscript_metadata\ndict JSON\nEval configuration metadata (e.g. task, model, scores, etc.).\n\n\nscan_id\nstr\nGlobally unique identifier for scan.\n\n\nscan_tags\nlist[str]\nJSON\nTags associated with the scan.\n\n\nscan_metadata\ndictJSON\nAdditional scan metadata.\n\n\nscanner_key\nstr\nUnique key for scan within scan job (defaults to scanner_name).\n\n\nscanner_name\nstr\nScanner name.\n\n\nscanner_file\nstr\nSource file for scanner.\n\n\nscanner_params\ndictJSON\nParams used to create scanner.\n\n\ninput_type\ntranscript | message | messages | event | events\nInput type received by scanner.\n\n\ninput_ids\nlist[str]JSON\nUnique ids of scanner input.\n\n\ninput\nScannerInputJSON\nScanner input value.\n\n\nvalue\nJsonValueJSON\nValue returned by scanner.\n\n\nvalue_type\nstring | boolean | number | array | object | null\nType of value returned by scanner.\n\n\nexplanation\nstr\nExplanation for scan result.\n\n\nmetadata\ndictJSON\nMetadata for scan result.\n\n\nmessage_references\nlist[str]JSON\nMessages referenced by scanner.\n\n\nevent_references\nlist[str]JSON\nEvents referenced by scanner.\n\n\nscan_error\nstr\nError which occurred during scan.\n\n\nscan_error_traceback\nstr\nTraceback for error (if any)\n\n\nscan_total_tokens\nnumber\nTotal tokens used by scan.\n\n\nscan_model_usage\ndict [str, ModelUsage]JSON\nToken usage by model for scan.\n\n\nscan_events\nlist[Event]JSON\nScan events (e.g. model event, log event, etc.)\n\n\n\nSeveral of these fields can be used to link back to the source eval log and sample for the transcript:\n\ntranscript_id — This is the same as the EvalSample.uuid in the Inspect log or the sample_id in data frames created by samples_df().\ntranscript_source_id — This is the same as the eval_id in both the Inspect log and Inspect data frames.\ntranscript_source_uri — This is the full path (filesystem or S3) to the actual log file where the transcript was read from.",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Results"
    ]
  },
  {
    "objectID": "scanners.html",
    "href": "scanners.html",
    "title": "Scanners",
    "section": "",
    "text": "Scanners are the main unit of processing in Inspect Scout and can target a wide variety of content types. In this article we’ll cover the basic scanning concepts, and then drill into creating scanners that target various types (Transcript, Event, or ChatMessage) as well as creating custom loaders which enable scanning of lists of events or messages.",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#overview",
    "href": "scanners.html#overview",
    "title": "Scanners",
    "section": "",
    "text": "Scanners are the main unit of processing in Inspect Scout and can target a wide variety of content types. In this article we’ll cover the basic scanning concepts, and then drill into creating scanners that target various types (Transcript, Event, or ChatMessage) as well as creating custom loaders which enable scanning of lists of events or messages.",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#scanner-basics",
    "href": "scanners.html#scanner-basics",
    "title": "Scanners",
    "section": "Scanner Basics",
    "text": "Scanner Basics\nA Scanner is a function that takes a ScannerInput (typically a Transcript, but possibly an Event, ChatMessage, or list of events or messages) and returns a Result. The result includes a value which can be of any type—this might be True to indicate that something was found but might equally be a number to indicate a count. More elaborate scanner values (dict or list) are also possible.\nHere is a simple scanner that uses a model to look for agent “confusion”—whether or not it finds confusion, it still returns the model completion as an explanation:\n@scanner(messages=\"all\")\ndef confusion() -&gt; Scanner[Transcript]:\n    \n    async def scan(transcript: Transcript) -&gt; Result:\n\n        # call model\n        output = await get_model().generate(\n            \"Here is a transcript of an LLM agent \" +\n            \"solving a puzzle:\\n\\n\" +\n            \"===================================\" +\n            messages_as_str(transcript.messages) +\n            \"===================================\\n\\n\" +\n            \"In the transcript above do you see the agent \" +\n            \"becoming confused? Repond beginning with 'Yes' \" +\n            \"or 'No', followed by an explanation.\"\n        )\n\n        # return result (value + full model completion)\n        return Result(\n            value=output.completion.lower().startswith(\"yes\"),\n            explanation=result.completion\n        )\n\n    return scan\nIf a scanner doesn’t find anything, it still returns a Result with value=None so that the explanation is preserved for later review.\n\nInput Types\nTranscript is the most common ScannerInput however several other types are possible:\n\nEvent — Single event from the transcript (e.g. ModelEvent, ToolEvent, etc.). More than one Event in a Transcript can be scanned.\nChatMessage — Single chat message from the transcript message history. More than one ChatMessage in a Transcript can be scanned.\nlist[Event] or list[ChatMessage] — Arbitrary sets of events or messages extracted from the Transcript (see Loaders below for details).\n\nSee the sections on Transcripts, Events, Messages, and Loaders below for additional details on handling various input types.\n\n\nInput Filtering\nOne important principle of the Inspect Scout transcript pipeline is that only the precise data to be scanned should be read, and nothing more. This can dramatically improve performance as messages and events that won’t be seen by scanners are never deserialized. Scanner input filters are specified as arguments to the @scanner decorator (you may have noticed the messages=\"all\" attached to the scanner decorator in the example above).\nFor example, here we are looking for instances of assistants swearing—for this task we only need to look at assistant messages so we specify messages=[\"assistant\"]\n@scanner(messages=[\"assistant\"])\ndef assistant_swearing() -&gt; Scanner[Transcript]:\n\n    async def scan(transcript: Transcript) -&gt; Result:\n        swear_words = [\n            word \n            for m in transcript.messages \n            for word in extract_swear_words(m.text)\n        ]\n        return Result(\n            value=swear_words if len(swear_words) &gt; 0 else None,\n            explanation=\",\".join(swear_words)\n        )\n\n    return scan\nWith this filter, only assistant messages and no events whatsoever will be loaded from transcripts during scanning.\nNote that by default, no filters are active, so if you don’t specify values for messages and/or events your scanner will not be called!",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#transcripts",
    "href": "scanners.html#transcripts",
    "title": "Scanners",
    "section": "Transcripts",
    "text": "Transcripts\nTranscripts are the most common input to scanners, and are read from one or more Inspect logs. A Transcript represents a single epoch from an Inspect sample—so each Inspect log file will have samples * epochs transcripts.\n\nTranscript Fields\nEach Transcript has the following fields:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nid\nstr\nGlobally unique identifier for a transcript (maps to EvalSample.uuid in the Inspect log).\n\n\nsource_id\nstr\nGlobally unique identifier for a transcript source (maps to eval_id in the Inspect log)\n\n\nsource_uri\nstr\nURI for source data (e.g. full path to the Inspect log file).\n\n\nmetadata\ndict[str, JsonValue]\nEval configuration metadata (e.g. task, model, scores, etc.). See LogMetadata for details.\n\n\nmessages\nlist[ChatMessage]\nMessage history from EvalSample\n\n\nevents\nlist[Event]\nEvent history from EvalSample\n\n\n\n\n\nContent Filtering\nNote that the messages and events fields will not be populated unless you specify a messages or events filter on your scanner. For example, this scanner will see all messages and events:\n@scanner(messages=\"all\", events=\"all\")\ndef my_scanner() -&gt; Scanner[Transcript]: ...\nThis scanner will see only model and tool events:\n@scanner(events=[\"model\", \"tool\"])\ndef my_scanner() -&gt; Scanner[Transcript]: ...\nThis scanner will see only assistant messages:\n@scanner(messages=[\"assistant\"])\ndef my_scanner() -&gt; Scanner[Transcript]: ...\n\n\nPresenting Messages\nWhen processing transcripts, you will often want to present an entire message history to model for analysis. Above, we used the messages_as_str() function to do this:\n# call model\nresult = await get_model().generate(\n    \"Here is a transcript of an LLM agent \" +\n    \"solving a puzzle:\\n\\n\" +\n    \"===================================\" +\n    messages_as_str(transcript.messages) +\n    \"===================================\\n\\n\" +\n    \"In the transcript above do you see the agent \" +\n    \"becoming confused? Repond beginning with 'Yes' \" +\n    \"or 'No', followed by an explanation.\"\n)",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#events",
    "href": "scanners.html#events",
    "title": "Scanners",
    "section": "Events",
    "text": "Events\nTo write a scanner that targets events, write a function that takes the event type(s) you want to process. For example, this scanner will see only model events:\n@scanner\ndef my_scanner() -&gt; Scanner[ModelEvent]:\n    def scan(event: ModelEvent) -&gt; Result: \n        ...\n\n    return scan\nNote that the events=\"model\" filter was not required since we had already declared our scanner to take only model events. If we wanted to take both model and tool events we’d do this:\n@scanner\ndef my_scanner() -&gt; Scanner[ModelEvent | ToolEvent]:\n    def scan(event: ModelEvent | ToolEvent) -&gt; Result: \n        ...\n\n    return scan",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#messages",
    "href": "scanners.html#messages",
    "title": "Scanners",
    "section": "Messages",
    "text": "Messages\nTo write a scanner that targets messages, write a function that takes the message type(s) you want to process. For example, this scanner will only see tool messages:\n@scanner\ndef my_scanner() -&gt; Scanner[ChatMessageTool]:\n    def scan(message: ChatMessageTool) -&gt; Result: \n        ...\n\n    return scan\nThis scanner will see only user and assistant messages:\n@scanner\ndef my_scanner() -&gt; Scanner[ChatMessageUser | ChatMessageAssistant]:\n    def scan(message: ChatMessageUser | ChatMessageAssistant) -&gt; Result: \n        ...\n\n    return scan",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "scanners.html#loaders",
    "href": "scanners.html#loaders",
    "title": "Scanners",
    "section": "Loaders",
    "text": "Loaders\nWhen you want to process multiple discrete items from a Transcript this might not always fall neatly into single messages or events. For example, you might want to process pairs of user/assistant messages. To do this, create a custom Loader that yields the content as required.\nFor example, here is a Loader that yields user/assistant message pairs:\n@loader(messages=[\"user\", \"assistant\"])\ndef conversation_turns():\n    async def load(\n        transcript: Transcript\n    ) -&gt; AsyncGenerator[list[ChatMessage], None]:\n        \n        for user,assistant in message_pairs(transcript.messages):\n            yield [user, assistant]\n\n    return load\nNote that just like with scanners, the loader still needs to provide a messages=[\"user\", \"assistant\"] in order to see those messages.\nWe can now use this loader in a scanner that looks for refusals:\n@scanner(loader=conversation_turns())\ndef assistant_refusals() -&gt; Scanner[list[ChatMessage]]:\n\n    async def scan(messages: list[ChatMessage]) -&gt; Result:\n        user, assistant = messages\n        return Result(\n            value=is_refusal(assistant.text), \n            explanation=messages_as_str(messages)\n        )\n\n    return scan",
    "crumbs": [
      "Getting Started",
      "Using Scout",
      "Scanners"
    ]
  },
  {
    "objectID": "reference/async.html",
    "href": "reference/async.html",
    "title": "Async API",
    "section": "",
    "text": "Note\n\n\n\nThe Async API is available for async programs that want to use inspect_scout as an embedded library. Normal usage of Scout (e.g. in a script or notebook) should prefer the corresponding sync functions (e.g. scan(), scan_resume()., etc.)\n\n\n\nscan_async\nScan transcripts.\nScan transcripts using one or more scanners. Note that scanners must each have a unique name. If you have more than one instance of a scanner with the same name, numbered prefixes will be automatically assigned. Alternatively, you can pass tuples of (name,scanner) or a dict with explicit names for each scanner.\n\nSource\n\nasync def scan_async(\n    scanners: Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]]\n    | dict[str, Scanner[ScannerInput]]\n    | ScanJob\n    | ScanJobConfig,\n    transcripts: Transcripts | None = None,\n    results: str | None = None,\n    model: str | Model | None = None,\n    model_config: GenerateConfig | None = None,\n    model_base_url: str | None = None,\n    model_args: dict[str, Any] | str | None = None,\n    model_roles: dict[str, str | Model] | None = None,\n    max_transcripts: int | None = None,\n    max_processes: int | None = None,\n    limit: int | None = None,\n    shuffle: bool | int | None = None,\n    tags: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscanners Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]] | dict[str, Scanner[ScannerInput]] | ScanJob | ScanJobConfig\n\nScanners to execute (list, dict with explicit names, or ScanJob). If a ScanJob or ScanJobConfig is specified, then its options are used as the default options for the scan.\n\ntranscripts Transcripts | None\n\nTranscripts to scan.\n\nresults str | None\n\nLocation to write results (filesystem or S3 bucket). Defaults to “./scans”.\n\nmodel str | Model | None\n\nModel to use for scanning by default (individual scanners can always call get_model() to us arbitrary models). If not specified use the value of the SCOUT_SCAN_MODEL environment variable.\n\nmodel_config GenerateConfig | None\n\nGenerationConfig for calls to the model.\n\nmodel_base_url str | None\n\nBase URL for communicating with the model API.\n\nmodel_args dict[str, Any] | str | None\n\nModel creation args (as a dictionary or as a path to a JSON or YAML config file).\n\nmodel_roles dict[str, str | Model] | None\n\nNamed roles for use in get_model().\n\nmax_transcripts int | None\n\nThe maximum number of transcripts to process concurrently (this also serves as the default value for max_connections). Defaults to 25.\n\nmax_processes int | None\n\nThe maximum number of concurrent processes (for multiproccesing). Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nLimit the number of transcripts processed.\n\nshuffle bool | int | None\n\nShuffle the order of transcripts (pass an int to set a seed for shuffling).\n\ntags list[str] | None\n\nOne or more tags for this scan.\n\nmetadata dict[str, Any] | None\n\nMetadata for this scan.\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\nscan_resume_async\nResume a previous scan.\n\nSource\n\nasync def scan_resume_async(scan_location: str, log_level: str | None = None) -&gt; Status\n\nscan_location str\n\nScan location to resume from.\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\nscan_complete_async\nComplete a scan.\nThis function is used to indicate that a scan with errors in some transcripts should be completed in spite of the errors.\n\nSource\n\nasync def scan_complete_async(\n    scan_location: str, log_level: str | None = None\n) -&gt; Status\n\nscan_location str\n\nScan location to complete.\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\nscan_list_async\nList completed and pending scans.\n\nSource\n\nasync def scan_list_async(scans_location: str) -&gt; list[Status]\n\nscans_location str\n\nLocation of scans to list.\n\n\n\n\nscan_status_async\nStatus of scan.\n\nSource\n\nasync def scan_status_async(scan_location: str) -&gt; Status\n\nscan_location str\n\nLocation to get status for (e.g. directory or s3 bucket)\n\n\n\n\nscan_results_async\nScan results as Pandas data frames.\n\nSource\n\nasync def scan_results_async(\n    scan_location: str, *, scanner: str | None = None\n) -&gt; Results\n\nscan_location str\n\nLocation of scan (e.g. directory or s3 bucket).\n\nscanner str | None\n\nScanner name (defaults to all scanners).\n\n\n\n\nscan_results_db_async\nScan results as DuckDB database.\n\nSource\n\nasync def scan_results_db_async(scan_location: str) -&gt; ResultsDB\n\nscan_location str\n\nLocation of scan (e.g. directory or s3 bucket).",
    "crumbs": [
      "Reference",
      "Python API",
      "async"
    ]
  },
  {
    "objectID": "reference/transcript.html",
    "href": "reference/transcript.html",
    "title": "Transcript API",
    "section": "",
    "text": "transcripts_from_logs\nRead sample transcripts from eval logs.\nLogs can be specified by file or directory path(s) or alternatively an evals_df() or samples_df()\n\nSource\n\ndef transcripts_from_logs(logs: LogPaths | pd.DataFrame) -&gt; Transcripts\n\nlogs LogPaths | pd.DataFrame\n\nLog paths as file(s), directories, or data frame.\n\n\n\n\nTranscripts\nCollection of transcripts for scanning.\nTranscript collections can be filtered using the where(), limit(), and ’shuffle()` methods. The transcripts are not modified in place so the filtered transcripts should be referenced via the return value. For example:\nfrom inspect_scout import transcripts, log_metadata as m\n\ntranscripts = transcripts_from_logs(\"./logs\")\ntranscripts = transcripts.where(m.task_name == \"cybench\")\n\nSource\n\nclass Transcripts(abc.ABC)\n\nMethods\n\nwhere\n\nFilter the transcript collection by a Condition.\n\nSource\n\ndef where(self, condition: Condition) -&gt; \"Transcripts\"\n\ncondition Condition\n\nFilter condition.\n\n\n\nlimit\n\nLimit the number of transcripts processed.\n\nSource\n\ndef limit(self, n: int) -&gt; \"Transcripts\"\n\nn int\n\nLimit on transcripts.\n\n\n\nshuffle\n\nShuffle the order of transcripts.\n\nSource\n\ndef shuffle(self, seed: int | None = None) -&gt; \"Transcripts\"\n\nseed int | None\n\nRandom seed for shuffling.\n\n\n\ncount\n\nNumber of transcripts in collection.\n\nSource\n\n@abc.abstractmethod\nasync def count(self) -&gt; int\n\n\n\n\nindex\n\nIndex of TranscriptInfo for the collection.\n\nSource\n\n@abc.abstractmethod\nasync def index(self) -&gt; Iterator[TranscriptInfo]\n\n\n\n\n\n\n\n\nTranscriptInfo\nTranscript identifier, location, and metadata.\n\nSource\n\nclass TranscriptInfo(BaseModel)\n\nAttributes\n\nid str\n\nGlobally unique id for transcript (e.g. sample uuid).\n\nsource_id str\n\nGlobally unique ID for transcript source (e.g. eval_id).\n\nsource_uri str\n\nURI for source data (e.g. log file path)\n\nmetadata dict[str, JsonValue]\n\ne.g. eval config (model, scores, task params, etc.).\n\n\n\n\n\nTranscript\nTranscript info and transcript content (messages and events).\n\nSource\n\nclass Transcript(TranscriptInfo)\n\nAttributes\n\nmessages list[ChatMessage]\n\nMain message thread.\n\nevents list[Event]\n\nEvents from transcript.\n\n\n\n\n\nColumn\nDatabase column with comparison operators.\nSupports various predicate functions including like(), not_like(), between(), etc. Additionally supports standard python equality and comparison operators (e.g. ==, ’&gt;`, etc.\n\nSource\n\nclass Column\n\nMethods\n\nin_\n\nCheck if value is in a list.\n\nSource\n\ndef in_(self, values: list[Any]) -&gt; Condition\n\nvalues list[Any]\n\n\n\n\n\nnot_in\n\nCheck if value is not in a list.\n\nSource\n\ndef not_in(self, values: list[Any]) -&gt; Condition\n\nvalues list[Any]\n\n\n\n\n\nlike\n\nSQL LIKE pattern matching (case-sensitive).\n\nSource\n\ndef like(self, pattern: str) -&gt; Condition\n\npattern str\n\n\n\n\n\nnot_like\n\nSQL NOT LIKE pattern matching (case-sensitive).\n\nSource\n\ndef not_like(self, pattern: str) -&gt; Condition\n\npattern str\n\n\n\n\n\nilike\n\nPostgreSQL ILIKE pattern matching (case-insensitive).\nNote: For SQLite and DuckDB, this will use LIKE with LOWER() for case-insensitivity.\n\nSource\n\ndef ilike(self, pattern: str) -&gt; Condition\n\npattern str\n\n\n\n\n\nnot_ilike\n\nPostgreSQL NOT ILIKE pattern matching (case-insensitive).\nNote: For SQLite and DuckDB, this will use NOT LIKE with LOWER() for case-insensitivity.\n\nSource\n\ndef not_ilike(self, pattern: str) -&gt; Condition\n\npattern str\n\n\n\n\n\nis_null\n\nCheck if value is NULL.\n\nSource\n\ndef is_null(self) -&gt; Condition\n\n\n\n\nis_not_null\n\nCheck if value is not NULL.\n\nSource\n\ndef is_not_null(self) -&gt; Condition\n\n\n\n\nbetween\n\nCheck if value is between two values.\n\nSource\n\ndef between(self, low: Any, high: Any) -&gt; Condition\n\nlow Any\n\nLower bound (inclusive). If None, raises ValueError.\n\nhigh Any\n\nUpper bound (inclusive). If None, raises ValueError.\n\n\n\nnot_between\n\nCheck if value is not between two values.\n\nSource\n\ndef not_between(self, low: Any, high: Any) -&gt; Condition\n\nlow Any\n\nLower bound (inclusive). If None, raises ValueError.\n\nhigh Any\n\nUpper bound (inclusive). If None, raises ValueError.\n\n\n\n\n\n\n\nCondition\nWHERE clause condition that can be combined with others.\n\nSource\n\nclass Condition\n\nMethods\n\nto_sql\n\nGenerate SQL WHERE clause and parameters.\n\nSource\n\ndef to_sql(\n    self,\n    dialect: Union[\n        SQLDialect, Literal[\"sqlite\", \"duckdb\", \"postgres\"]\n    ] = SQLDialect.SQLITE,\n) -&gt; tuple[str, list[Any]]\n\ndialect Union[SQLDialect, Literal['sqlite', 'duckdb', 'postgres']]\n\nTarget SQL dialect (sqlite, duckdb, or postgres).\n\n\n\n\n\n\n\nMetadata\nEntry point for building metadata filter expressions.\n\nSource\n\nclass Metadata\n\n\nmetadata\nMetadata selector for where expressions.\nTypically aliased to a more compact expression (e.g. m) for use in queries). For example:\nfrom inspect_scout import metadata as m\nfilter = m.model == \"gpt-4\"\nfilter = (m.task_name == \"math\") & (m.epochs &gt; 1)\n\nSource\n\nmetadata = Metadata()\n\n\nLogMetadata\nTyped metadata interface for Inspect log transcripts.\nProvides typed properties for standard Inspect log columns while preserving the ability to access custom fields through the base Metadata class methods.\n\nSource\n\nclass LogMetadata(Metadata)\n\nAttributes\n\nsample_id Column\n\nUnique id for sample.\n\neval_id Column\n\nGlobally unique id for eval.\n\nlog Column\n\nLocation that the log file was read from.\n\neval_created Column\n\nTime eval was created.\n\neval_tags Column\n\nTags associated with evaluation run.\n\neval_metadata Column\n\nAdditional eval metadata.\n\ntask_name Column\n\nTask name.\n\ntask_args Column\n\nTask arguments.\n\nsolver Column\n\nSolver name.\n\nsolver_args Column\n\nArguments used for invoking the solver.\n\nmodel Column\n\nModel used for eval.\n\ngenerate_config Column\n\nGenerate config specified for model instance.\n\nmodel_roles Column\n\nModel roles.\n\nid Column\n\nUnique id for sample.\n\nepoch Column\n\nEpoch number for sample.\n\nsample_metadata Column\n\nSample metadata.\n\nscore Column\n\nHeadline score value.\n\ntotal_tokens Column\n\nTotal tokens used for sample.\n\ntotal_time Column\n\nTotal time that the sample was running.\n\nworking_time Column\n\nTime spent working (model generation, sandbox calls, etc.).\n\nerror Column\n\nError that halted the sample.\n\nlimit Column\n\nLimit that halted the sample.\n\n\n\n\n\nlog_metadata\nLog metadata selector for where expressions.\nTypically aliased to a more compact expression (e.g. m) for use in queries). For example:\nfrom inspect_scout import log_metadata as m\n\n# typed access to standard fields\nfilter = m.model == \"gpt-4\"\nfilter = (m.task_name == \"math\") & (m.epochs &gt; 1)\n\n# dynamic access to custom fields\nfilter = m[\"custom_field\"] &gt; 100\n\nSource\n\nlog_metadata = LogMetadata()",
    "crumbs": [
      "Reference",
      "Python API",
      "transcript"
    ]
  },
  {
    "objectID": "reference/results.html",
    "href": "reference/results.html",
    "title": "Results",
    "section": "",
    "text": "scan_list\nList completed and pending scans.\n\nSource\n\ndef scan_list(scans_location: str) -&gt; list[Status]\n\nscans_location str\n\nLocation of scans to list.\n\n\n\n\nscan_status\nStatus of scan.\n\nSource\n\ndef scan_status(scan_location: str) -&gt; Status\n\nscan_location str\n\nLocation to get status for (e.g. directory or s3 bucket)\n\n\n\n\nscan_results\nScan results as Pandas data frames.\n\nSource\n\ndef scan_results(scan_location: str, *, scanner: str | None = None) -&gt; Results\n\nscan_location str\n\nLocation of scan (e.g. directory or s3 bucket).\n\nscanner str | None\n\nScanner name (defaults to all scanners).\n\n\n\n\nscan_results_db\nScan results as DuckDB database.\n\nSource\n\ndef scan_results_db(scan_location: str) -&gt; ResultsDB\n\nscan_location str\n\nLocation of scan (e.g. directory or s3 bucket).\n\n\n\n\nStatus\nStatus of scan job.\n\nSource\n\n@dataclass\nclass Status\n\nAttributes\n\ncomplete bool\n\nIs the job complete (all transcripts scanned).\n\nspec ScanSpec\n\nScan spec (transcripts, scanners, options).\n\nlocation str\n\nLocation of scan directory.\n\nsummary Summary\n\nSummary of scan (results, errors, tokens, etc.)\n\nerrors list[Error]\n\nErrors during last scan attempt.\n\n\n\n\n\nSummary\nSummary of scan results.\n\nSource\n\nclass Summary(BaseModel)\n\nAttributes\n\nscanners dict[str, ScannerSummary]\n\nSummary for each scanner.\n\n\n\n\n\nResults\nScan results as pandas data frames.\n\nSource\n\n@dataclass\nclass Results(Status)\n\nAttributes\n\nscanners dict[str, pd.DataFrame]\n\nDict of scanner name to pandas data frame.\n\n\n\n\n\nResultsDB\nScan results as DuckDB database.\nUse ScanResultsDB as a context manager to close the DuckDb connection when you are finished using it.\nUse the to_file() method to create a DuckDB database file for the results.\n\nSource\n\n@dataclass\nclass ResultsDB(Status)\n\nAttributes\n\nconn duckdb.DuckDBPyConnection\n\nConnection to DuckDB database.\n\n\n\n\nMethods\n\nto_file\n\nWrite the database contents to a DuckDB file.\nThis materializes all views and tables from the in-memory connection into a persistent DuckDB database file.\n\nSource\n\ndef to_file(self, file: str, overwrite: bool = False) -&gt; None\n\nfile str\n\nFile where the DuckDB database file should be written. Supports local paths, S3 URIs (s3://bucket/path), and GCS URIs (gs://bucket/path or gcs://bucket/path).\n\noverwrite bool\n\nIf True, overwrite existing file. If False (default), raise FileExistsError if file already exists.",
    "crumbs": [
      "Reference",
      "Python API",
      "results"
    ]
  },
  {
    "objectID": "reference/scanning.html",
    "href": "reference/scanning.html",
    "title": "Scanning",
    "section": "",
    "text": "Scan transcripts.\nScan transcripts using one or more scanners. Note that scanners must each have a unique name. If you have more than one instance of a scanner with the same name, numbered prefixes will be automatically assigned. Alternatively, you can pass tuples of (name,scanner) or a dict with explicit names for each scanner.\n\nSource\n\ndef scan(\n    scanners: Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]]\n    | dict[str, Scanner[ScannerInput]]\n    | ScanJob\n    | ScanJobConfig,\n    transcripts: Transcripts | None = None,\n    results: str | None = None,\n    model: str | Model | None = None,\n    model_config: GenerateConfig | None = None,\n    model_base_url: str | None = None,\n    model_args: dict[str, Any] | str | None = None,\n    model_roles: dict[str, str | Model] | None = None,\n    max_transcripts: int | None = None,\n    max_processes: int | None = None,\n    limit: int | None = None,\n    shuffle: bool | int | None = None,\n    tags: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscanners Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]] | dict[str, Scanner[ScannerInput]] | ScanJob | ScanJobConfig\n\nScanners to execute (list, dict with explicit names, or ScanJob). If a ScanJob or ScanJobConfig is specified, then its options are used as the default options for the scan.\n\ntranscripts Transcripts | None\n\nTranscripts to scan.\n\nresults str | None\n\nLocation to write results (filesystem or S3 bucket). Defaults to “./scans”.\n\nmodel str | Model | None\n\nModel to use for scanning by default (individual scanners can always call get_model() to us arbitrary models). If not specified use the value of the SCOUT_SCAN_MODEL environment variable.\n\nmodel_config GenerateConfig | None\n\nGenerationConfig for calls to the model.\n\nmodel_base_url str | None\n\nBase URL for communicating with the model API.\n\nmodel_args dict[str, Any] | str | None\n\nModel creation args (as a dictionary or as a path to a JSON or YAML config file).\n\nmodel_roles dict[str, str | Model] | None\n\nNamed roles for use in get_model().\n\nmax_transcripts int | None\n\nThe maximum number of transcripts to process concurrently (this also serves as the default value for max_connections). Defaults to 25.\n\nmax_processes int | None\n\nThe maximum number of concurrent processes (for multiproccesing). Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nLimit the number of transcripts processed.\n\nshuffle bool | int | None\n\nShuffle the order of transcripts (pass an int to set a seed for shuffling).\n\ntags list[str] | None\n\nOne or more tags for this scan.\n\nmetadata dict[str, Any] | None\n\nMetadata for this scan.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\n\nResume a previous scan.\n\nSource\n\ndef scan_resume(\n    scan_location: str,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscan_location str\n\nScan location to resume from.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\n\nComplete a scan.\nThis function is used to indicate that a scan with errors in some transcripts should be completed in spite of the errors.\n\nSource\n\ndef scan_complete(\n    scan_location: str,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscan_location str\n\nScan location to complete.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/scanning.html#scanning",
    "href": "reference/scanning.html#scanning",
    "title": "Scanning",
    "section": "",
    "text": "Scan transcripts.\nScan transcripts using one or more scanners. Note that scanners must each have a unique name. If you have more than one instance of a scanner with the same name, numbered prefixes will be automatically assigned. Alternatively, you can pass tuples of (name,scanner) or a dict with explicit names for each scanner.\n\nSource\n\ndef scan(\n    scanners: Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]]\n    | dict[str, Scanner[ScannerInput]]\n    | ScanJob\n    | ScanJobConfig,\n    transcripts: Transcripts | None = None,\n    results: str | None = None,\n    model: str | Model | None = None,\n    model_config: GenerateConfig | None = None,\n    model_base_url: str | None = None,\n    model_args: dict[str, Any] | str | None = None,\n    model_roles: dict[str, str | Model] | None = None,\n    max_transcripts: int | None = None,\n    max_processes: int | None = None,\n    limit: int | None = None,\n    shuffle: bool | int | None = None,\n    tags: list[str] | None = None,\n    metadata: dict[str, Any] | None = None,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscanners Sequence[Scanner[ScannerInput] | tuple[str, Scanner[ScannerInput]]] | dict[str, Scanner[ScannerInput]] | ScanJob | ScanJobConfig\n\nScanners to execute (list, dict with explicit names, or ScanJob). If a ScanJob or ScanJobConfig is specified, then its options are used as the default options for the scan.\n\ntranscripts Transcripts | None\n\nTranscripts to scan.\n\nresults str | None\n\nLocation to write results (filesystem or S3 bucket). Defaults to “./scans”.\n\nmodel str | Model | None\n\nModel to use for scanning by default (individual scanners can always call get_model() to us arbitrary models). If not specified use the value of the SCOUT_SCAN_MODEL environment variable.\n\nmodel_config GenerateConfig | None\n\nGenerationConfig for calls to the model.\n\nmodel_base_url str | None\n\nBase URL for communicating with the model API.\n\nmodel_args dict[str, Any] | str | None\n\nModel creation args (as a dictionary or as a path to a JSON or YAML config file).\n\nmodel_roles dict[str, str | Model] | None\n\nNamed roles for use in get_model().\n\nmax_transcripts int | None\n\nThe maximum number of transcripts to process concurrently (this also serves as the default value for max_connections). Defaults to 25.\n\nmax_processes int | None\n\nThe maximum number of concurrent processes (for multiproccesing). Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nLimit the number of transcripts processed.\n\nshuffle bool | int | None\n\nShuffle the order of transcripts (pass an int to set a seed for shuffling).\n\ntags list[str] | None\n\nOne or more tags for this scan.\n\nmetadata dict[str, Any] | None\n\nMetadata for this scan.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\n\nResume a previous scan.\n\nSource\n\ndef scan_resume(\n    scan_location: str,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscan_location str\n\nScan location to resume from.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)\n\n\n\n\n\nComplete a scan.\nThis function is used to indicate that a scan with errors in some transcripts should be completed in spite of the errors.\n\nSource\n\ndef scan_complete(\n    scan_location: str,\n    display: DisplayType | None = None,\n    log_level: str | None = None,\n) -&gt; Status\n\nscan_location str\n\nScan location to complete.\n\ndisplay DisplayType | None\n\nDisplay type: “rich”, “plain”, or “none” (defaults to “rich”).\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”)",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/scanning.html#jobs",
    "href": "reference/scanning.html#jobs",
    "title": "Scanning",
    "section": "Jobs",
    "text": "Jobs\n\nscanjob\nDecorator for registering scan jobs.\n\nSource\n\ndef scanjob(\n    func: ScanJobType | None = None, *, name: str | None = None\n) -&gt; ScanJobType | Callable[[ScanJobType], ScanJobType]\n\nfunc ScanJobType | None\n\nFunction returning ScanJob.\n\nname str | None\n\nOptional name for scanjob (defaults to function name).\n\n\n\n\nScanJob\nScan job definition.\n\nSource\n\nclass ScanJob\n\nAttributes\n\nname str\n\nName of scan job (defaults to @scanjob function name).\n\ntranscripts Transcripts | None\n\nTrasnscripts to scan.\n\nscanners dict[str, Scanner[ScannerInput]]\n\nScanners to apply to transcripts.\n\nresults str | None\n\nLocation to write results (filesystem or S3 bucket). Defaults to “./scans”.\n\nmodel Model | None\n\nModel to use for scanning by default (individual scanners can always call get_model() to us arbitrary models).\nIf not specified use the value of the SCOUT_SCAN_MODEL environment variable.\n\nmodel_base_url str | None\n\nBase URL for communicating with the model API.\n\nmodel_args dict[str, Any] | None\n\nModel creation args (as a dictionary or as a path to a JSON or YAML config file).\n\ngenerate_config GenerateConfig | None\n\nGenerationConfig for calls to the model.\n\nmodel_roles dict[str, Model] | None\n\nNamed roles for use in get_model().\n\nmax_transcripts int | None\n\nThe maximum number of transcripts to process concurrently (this also serves as the default value for max_connections). Defaults to 25.\n\nmax_processes int | None\n\nThe maximum number of concurrent processes (for multiproccesing). Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nLimit the number of transcripts processed.\n\nshuffle bool | int | None\n\nShuffle the order of transcripts (pass an int to set a seed for shuffling).\n\ntags list[str] | None\n\nOne or more tags for this scan.\n\nmetadata dict[str, Any] | None\n\nMetadata for this scan.\n\nlog_level str | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\n\n\n\n\nScanJobConfig\nScan job configuration.\n\nSource\n\nclass ScanJobConfig(BaseModel)\n\nAttributes\n\nname str\n\nName of scan job (defaults to “job”).\n\ntranscripts str | list[str] | None\n\nTrasnscripts to scan.\n\nscanners list[ScannerSpec] | dict[str, ScannerSpec] | None\n\nScanners to apply to transcripts.\n\nresults str | None\n\nLocation to write results (filesystem or S3 bucket). Defaults to “./scans”.\n\nmodel str | None\n\nModel to use for scanning by default (individual scanners can always call get_model() to us arbitrary models).\nIf not specified use the value of the SCOUT_SCAN_MODEL environment variable.\n\nmodel_base_url str | None\n\nBase URL for communicating with the model API.\n\nmodel_args dict[str, Any] | str | None\n\nModel creation args (as a dictionary or as a path to a JSON or YAML config file).\n\ngenerate_config GenerateConfig | None\n\nGenerationConfig for calls to the model.\n\nmodel_roles dict[str, ModelConfig | str] | None\n\nNamed roles for use in get_model().\n\nmax_transcripts int | None\n\nThe maximum number of transcripts to process concurrently (this also serves as the default value for max_connections). Defaults to 25.\n\nmax_processes int | None\n\nThe maximum number of concurrent processes (for multiproccesing). Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nLimit the number of transcripts processed.\n\nshuffle bool | int | None\n\nShuffle the order of transcripts (pass an int to set a seed for shuffling).\n\ntags list[str] | None\n\nOne or more tags for this scan.\n\nmetadata dict[str, Any] | None\n\nMetadata for this scan.\n\nlog_level Literal['debug', 'http', 'sandbox', 'info', 'warning', 'error', 'critical', 'notset'] | None\n\nLevel for logging to the console: “debug”, “http”, “sandbox”, “info”, “warning”, “error”, “critical”, or “notset” (defaults to “warning”).\n\n\n\n\n\nScannerSpec\nScanner used by scan.\n\nSource\n\nclass ScannerSpec(BaseModel)\n\nAttributes\n\nname str\n\nScanner name.\n\nfile str | None\n\nScanner source file (if not in a package).\n\nparams dict[str, Any]\n\nScanner arguments.",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  },
  {
    "objectID": "reference/scanning.html#status",
    "href": "reference/scanning.html#status",
    "title": "Scanning",
    "section": "Status",
    "text": "Status\n\nStatus\nStatus of scan job.\n\nSource\n\n@dataclass\nclass Status\n\nAttributes\n\ncomplete bool\n\nIs the job complete (all transcripts scanned).\n\nspec ScanSpec\n\nScan spec (transcripts, scanners, options).\n\nlocation str\n\nLocation of scan directory.\n\nsummary Summary\n\nSummary of scan (results, errors, tokens, etc.)\n\nerrors list[Error]\n\nErrors during last scan attempt.\n\n\n\n\n\nScanOptions\nOptions used for scan.\n\nSource\n\nclass ScanOptions(BaseModel)\n\nAttributes\n\nmax_transcripts int\n\nMaximum number of concurrent transcripts (defaults to 25).\n\nmax_processes int\n\nNumber of worker processes. Defaults to multiprocessing.cpu_count().\n\nlimit int | None\n\nTranscript limit (maximum number of transcripts to read).\n\nshuffle bool | int | None\n\nShuffle order of transcripts.\n\n\n\n\n\nScanRevision\nGit revision for scan.\n\nSource\n\nclass ScanRevision(BaseModel)\n\nAttributes\n\ntype Literal['git']\n\nType of revision (currently only “git”)\n\norigin str\n\nRevision origin server\n\ncommit str\n\nRevision commit.\n\n\n\n\n\nScanTranscripts\nTranscripts target by a scan.\n\nSource\n\nclass ScanTranscripts(BaseModel)\n\nAttributes\n\ntype str\n\nTranscripts backing store type (currently only ‘eval_log’).\n\nfields list[TranscriptField]\n\nData types of transcripts fields.\n\ncount int\n\nTrancript count.\n\ndata str\n\nTranscript data as a csv.\n\n\n\n\n\nTranscriptField\nField in transcript data frame.\n\nSource\n\nclass TranscriptField(TypedDict, total=False)\n\nAttributes\n\nname Required[str]\n\nField name.\n\ntype Required[str]\n\nField type (“integer”, “number”, “boolean”, “string”, or “datetime”)\n\ntz NotRequired[str]\n\nTimezone (for “datetime” fields).",
    "crumbs": [
      "Reference",
      "Python API"
    ]
  }
]